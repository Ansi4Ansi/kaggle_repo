{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install accelerate==0.25.0\n#!pip install bertopic==0.15.0\n#!pip install datasets==2.14.4\n#!pip install faiss-cpu==1.7.4\n#!pip install langchain==0.0.348\n#!pip install langchainhub==0.1.14\n#!pip install sentence-transformers==2.2.2\n#!pip install sentencepiece==0.1.99\n#!pip install transformers==4.24.0\n!pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport transformers\nimport typing as tp\nimport wandb\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, PreTrainedTokenizer, PreTrainedTokenizerFast\nfrom datasets import load_dataset\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import recall_score, f1_score, precision_score\nfrom pprint import pprint\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntorch.manual_seed(42)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    This class designs logic to retrieve data from a custom dataset.\n    According to pytorch Dataset conception any map style dataset\n    should implement at least __len__ and __getitem__ methods.\n    \"\"\"\n\n    def __init__(\n        self, texts, labels, tokenizer, max_length\n    ) -> None:\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self) -> int:\n        \"\"\"\n        returns number of rows in data\n        \"\"\"\n        return len(self.texts)\n\n    def __getitem__(self, idx: int) -> tp.Dict[str, tp.Any]:\n        \"\"\"\n        retrieves data for single index.\n        may include data processing and transformations.\n        E.g. augmenting data or tokenizing it.\n        returns dict with keys \"input_ids\", \"label\" and probably some more metadata (you decide whethere you need something more here)\n        \"\"\"\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\nclass ModelTrainer:\n    \"\"\"\n    This class implements logic run an experiemnt with a provided transformers classification model.\n    It incudes following components:\n    - load data\n    - load and configure a model and its artifacts\n    - train model\n    - validate model\n    - save model\n    - compue metrics\n    - run_experiment (as the man entrypoint to execute all flow)\n\n    Attention: current module intentionally doesnt support model inference or model serving.\n    It is a good practice to separate train/inference classes otherwise it is hard to maintain it all.\n\n    \"\"\"\n\n    def __init__(self, model_name: str, dataset_name: str) -> None:\n        self.model_name = model_name\n        self.dataset_name = dataset_name\n        \n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.model = transformers.AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=2)\n        \n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n        self.model_config = transformers.AutoConfig.from_pretrained(self.model_name)\n     \n\n    def configure_optimizer(self, optimizer, params: tp.Dict) -> None:\n        \"\"\"\n        adds a self.optimizer attribute with a chosen optimizer and its params.\n        \"\"\"\n\n        self.optimizer = optimizer\n        for key, value in params.items():\n            for g in self.optimizer.param_groups:\n                g[key] = value\n\n    def configure_scheduler(self, scheduler) -> None:\n        \"\"\"\n        adds a self.scheduler attribute with a chosen scheduler (e.g. ReduceLROnPlateau).\n        \"\"\"\n\n        self.scheduler = scheduler\n\n    def apply_data_parallel(self) -> None:\n        \"\"\"\n        checks number of available cuda devices,\n        if number of GPUs is > 1, moves self.model to a DataParallel state for faster training.\n        \"\"\"\n        \n        if torch.cuda.device_count()>1:\n            self.model = torch.nn.DataParallel(self.model, device_ids=list(range(torch.cuda.device_count())))\n\n    def load_data(self, filename: str, split: str) -> pd.DataFrame:\n        \"\"\"\n        uses Datasets library to load a dataset, takes as input dataset name (e.g. \"imdb\")\n        and a split. Loads data into pandas.\n        \"\"\"\n        \n        ds = load_dataset(filename, split=split)\n        return ds.to_pandas()\n\n    def train(self, dataset: CustomDataset) -> None:\n        \n        # заглушка, если шедулер и оптимайзер не инцииализированы\n        self.optimizer = self.optimizer or AdamW(self.model.parameters(), lr = 1e-3)\n        self.scheduler = self.scheduler or ExponentialLR(optimizer=self.optimizer, gamma=0.9, last_epoch=-1)\n        \n        train_loader = DataLoader(dataset, batch_size=8, shuffle=False)\n        \n        self.model.to(self.device)\n        \n        correct = 0\n        self.model.train()\n        for epoch in tqdm(range(wandb.config[\"epochs\"])):\n            total_loss = 0.0\n\n            for batch in train_loader:\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['label'].to(self.device)\n\n                self.optimizer.zero_grad()\n                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                loss.backward()\n                self.optimizer.step()\n                \n                if epoch % wandb.config[\"learning_rate_decay_step\"] == 0:\n                    self.scheduler.step()\n                #correct += (outputs[0].detach().cpu().numpy()==labels.to('cpu').numpy()).sum().item()\n                #pprint(correct)\n                total_loss += loss.item()\n            average_loss = total_loss / len(train_loader)\n            #average_accuracy = correct / len(train_loader)\n            wandb.log({\"accuracy\": average_accuracy, \"loss\": average_loss})\n            wandb.log({\"loss\": average_loss})\n            #print(f'Epoch {epoch+1}/{wandb.config[\"epochs\"]} - Average Loss: {average_loss:.4f} - Average accuracy {average_accuracy:.4f}')\n            print(f'Epoch {epoch+1}/{wandb.config[\"epochs\"]} - Average Loss: {average_loss:.4f}')\n\n        print(\"Training complete!\")\n\n    def validate(self, dataset: CustomDataset) -> tp.Dict[str, tp.Iterable]:\n        \"\"\"\n        takes a trained model and runs it on validation data.\n        Returns a dict with the keys \"valid_labels\" and \"valid_preds\" and corresponding values.\n        \"\"\"\n        \n        dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n        \n        model.to(self.device)\n        model.eval()\n        \n        valid_preds, valid_labels = [], []\n\n        for batch in dataloader:\n\n            b_input_ids = batch[\"input_ids\"].to(self.device)\n            b_input_mask = batch[\"attention_mask\"].to(self.device)\n            b_labels = batch[\"label\"].to(self.device)\n\n            with torch.no_grad():\n                logits = self.model(input_ids=b_input_ids, attention_mask=b_input_mask)\n\n            logits = logits[0].detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            batch_preds = np.argmax(logits, axis=1)\n            batch_labels = np.concatenate(label_ids.reshape(-1,1))\n            valid_preds.extend(batch_preds)\n            valid_labels.extend(batch_labels)\n\n        return valid_labels, valid_preds\n\n    def compute_metrics_report(\n        self, labels: tp.Iterable, predictions: tp.Iterable\n    ) -> tp.Any:\n        \"\"\"\n        Computes classification metric (or several metrcis) for given task.\n        \"\"\"\n        \n        recall = recall_score(true_labels, true_predictions)\n        precision = precision_score(true_labels, true_predictions)\n        f1_score = f1_score(true_labels, true_predictions)\n\n        results = {\n            'recall': recall,\n            'precision': precision,\n            'f1': f1_score\n        }\n        return results\n\n    def save_model(self, dst_path: str) -> None:\n        \"\"\"\n        Saves model to dst_path. Be careful to check if a model is on DataParallel state.\n        If it is, one needs to process it accordingly.\n        \"\"\"\n        if isinstance(self.model, nn.DataParallel):\n            torch.save(model.module.state_dict(), PATH)\n        else:\n            torch.save(self.model, dst_path)\n\n    def run_experiment(self):\n        \"\"\"\n        Main entrypoint.\n        Runs the flow from loading data to computing metrics.\n        \"\"\"\n        wandb.login(key = 'ed5e812f0a2ec095a0e7b29e696ac3f9655e62ed')\n        \n        train_df = self.load_data(filename = self.dataset_name, split = 'train')\n   \n        train_dataset = CustomDataset(\n            texts=train_df[\"text\"].tolist(),\n            labels=train_df[\"label\"].tolist(),\n            tokenizer=self.tokenizer,\n            max_length=512,\n        )\n        # Соберем общий конфиг для wandb\n        wandb_config = {\n            \"epochs\":5, \n            \"lr\": 1e-3, \n            \"weight_decay\": 5e-4, \n            \"learning_rate_decay_step\": 2, \n            \"learning_rate_decay_factor\": 0.9\n            ,}\n        \n        # Init для wandb\n        wandb.init(project=\"hw-07-transformers\", \n                   notes=\"Dataset Name \" + self.dataset_name, \n                   tags=[\"baseline\", \"transformer\", \"distilbert-base\"], \n                   config=wandb_config,\n                  )\n        # Для оптимайзера - срез из конфига по wandb        \n        optimizer_param_list = ['lr', 'weight_decay']\n        \n        optimizer_params = {k: wandb_config[k] for k in optimizer_param_list}\n        self.configure_optimizer(optimizer = AdamW(self.model.parameters()), params = optimizer_params)\n        \n        # Выбираем экспоненциальный шедулер\n        self.configure_scheduler(scheduler = ExponentialLR(optimizer=self.optimizer, gamma=wandb.config[\"learning_rate_decay_factor\"], last_epoch=-1))\n        \n        self.apply_data_parallel()\n        \n        self.train(train_dataset)\n        \n        test_df = self.load_data(filename = self.dataset_name, split = 'train')\n        \n        test_dataset = CustomDataset(\n            texts=test_df[\"text\"].tolist(),\n            labels=test_df[\"label\"].tolist(),\n            tokenizer=self.tokenizer,\n            max_length=512,\n        )\n        \n        \n        valid_labels, valid_preds = self.validate(test_dataset)\n        \n        valid_scores = self.compute_metrics_report(valid_labels, valid_preds)\n        \n        self.save_model(dst_path = './distilbert-base-uncased_baseline.pt')\n        \n        pprint(valid_scores)\n        \n\nif __name__ == \"__main__\":\n    \"\"\"run experiment\"\"\"\n    model_trainer = ModelTrainer(model_name = \"distilbert-base-uncased\",\n                                 dataset_name = \"imdb\", \n                                )\n    model_trainer.run_experiment()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}