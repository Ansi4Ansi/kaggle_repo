{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install accelerate==0.25.0\n#!pip install bertopic==0.15.0\n#!pip install datasets==2.14.4\n#!pip install faiss-cpu==1.7.4\n#!pip install langchain==0.0.348\n#!pip install langchainhub==0.1.14\n#!pip install sentence-transformers==2.2.2\n#!pip install sentencepiece==0.1.99\n#!pip install transformers==4.24.0\n!pip install wandb --upgrade","metadata":{"execution":{"iopub.status.busy":"2024-05-20T09:50:26.411292Z","iopub.execute_input":"2024-05-20T09:50:26.411907Z","iopub.status.idle":"2024-05-20T09:50:43.654054Z","shell.execute_reply.started":"2024-05-20T09:50:26.411876Z","shell.execute_reply":"2024-05-20T09:50:43.652843Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nCollecting wandb\n  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nDownloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.16.6\n    Uninstalling wandb-0.16.6:\n      Successfully uninstalled wandb-0.16.6\nSuccessfully installed wandb-0.17.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport transformers\nimport typing as tp\nimport wandb\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, PreTrainedTokenizer, PreTrainedTokenizerFast\nfrom datasets import load_dataset\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import recall_score, f1_score, precision_score\nfrom pprint import pprint\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntorch.manual_seed(42)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T09:50:43.656407Z","iopub.execute_input":"2024-05-20T09:50:43.657231Z","iopub.status.idle":"2024-05-20T09:50:51.029724Z","shell.execute_reply.started":"2024-05-20T09:50:43.657190Z","shell.execute_reply":"2024-05-20T09:50:51.028741Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7edc1747a0b0>"},"metadata":{}}]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    This class designs logic to retrieve data from a custom dataset.\n    According to pytorch Dataset conception any map style dataset\n    should implement at least __len__ and __getitem__ methods.\n    \"\"\"\n\n    def __init__(\n        self, texts, labels, tokenizer, max_length\n    ) -> None:\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self) -> int:\n        \"\"\"\n        returns number of rows in data\n        \"\"\"\n        return len(self.texts)\n\n    def __getitem__(self, idx: int) -> tp.Dict[str, tp.Any]:\n        \"\"\"\n        retrieves data for single index.\n        may include data processing and transformations.\n        E.g. augmenting data or tokenizing it.\n        returns dict with keys \"input_ids\", \"label\" and probably some more metadata (you decide whethere you need something more here)\n        \"\"\"\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\nclass ModelTrainer:\n    \"\"\"\n    This class implements logic run an experiemnt with a provided transformers classification model.\n    It incudes following components:\n    - load data\n    - load and configure a model and its artifacts\n    - train model\n    - validate model\n    - save model\n    - compue metrics\n    - run_experiment (as the man entrypoint to execute all flow)\n\n    Attention: current module intentionally doesnt support model inference or model serving.\n    It is a good practice to separate train/inference classes otherwise it is hard to maintain it all.\n\n    \"\"\"\n\n    def __init__(self, model_name: str, dataset_name: str, sample_size: int =-1, random_state: int = 42) -> None:\n        self.model_name = model_name\n        self.dataset_name = dataset_name\n        \n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.model = transformers.AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=2)\n        \n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n        self.model_config = transformers.AutoConfig.from_pretrained(self.model_name)\n        self.sample_size = sample_size\n        self.random_state = random_state\n\n    def configure_optimizer(self, optimizer, params: tp.Dict) -> None:\n        \"\"\"\n        adds a self.optimizer attribute with a chosen optimizer and its params.\n        \"\"\"\n\n        self.optimizer = optimizer\n        for key, value in params.items():\n            for g in self.optimizer.param_groups:\n                g[key] = value\n                \n        #pprint(optimizer.param_groups)\n\n    def configure_scheduler(self, scheduler) -> None:\n        \"\"\"\n        adds a self.scheduler attribute with a chosen scheduler (e.g. ReduceLROnPlateau).\n        \"\"\"\n\n        self.scheduler = scheduler\n\n    def apply_data_parallel(self) -> None:\n        \"\"\"\n        checks number of available cuda devices,\n        if number of GPUs is > 1, moves self.model to a DataParallel state for faster training.\n        \"\"\"\n        \n        if torch.cuda.device_count()>1:\n            self.model = torch.nn.DataParallel(self.model, device_ids=list(range(torch.cuda.device_count())))\n\n    def load_data(self, filename: str, split: str, sample_size: int = -1) -> pd.DataFrame:\n        \"\"\"\n        uses Datasets library to load a dataset, takes as input dataset name (e.g. \"imdb\")\n        and a split. Loads data into pandas.\n        \"\"\"\n        \n        ds = load_dataset(filename, split = split)\n        if sample_size> 0 :\n            return ds.to_pandas().sample(n = sample_size, random_state = self.random_state)\n        else:\n            return ds.to_pandas()\n        \n    def train(self, dataset: CustomDataset) -> None:\n        \n        # заглушка, если шедулер и оптимайзер не инцииализированы\n        self.optimizer = self.optimizer or AdamW(self.model.parameters(), lr = 1e-3)\n        self.scheduler = self.scheduler or ExponentialLR(optimizer=self.optimizer, gamma=0.9, last_epoch=-1)\n        \n        train_loader = DataLoader(dataset, batch_size=8, shuffle=False)\n        \n        self.model.to(self.device)\n        \n        correct = 0\n        self.model.train()\n        for epoch in tqdm(range(wandb.config[\"epochs\"])):\n            total_loss = 0.0\n\n            for batch in train_loader:\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['label'].to(self.device)\n\n                self.optimizer.zero_grad()\n                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                loss.backward()\n                self.optimizer.step()\n                \n                \n                #correct += (outputs[0].detach().cpu().numpy()==labels.to('cpu').numpy()).sum().item()\n                #pprint(correct)\n                total_loss += loss.item()\n            if epoch % wandb.config[\"learning_rate_decay_step\"] == 0:    \n                self.scheduler.step()\n            average_loss = total_loss / len(train_loader)\n            #average_accuracy = correct / len(train_loader)\n            #wandb.log({\"accuracy\": average_accuracy, \"loss\": average_loss})\n            wandb.log({\"loss\": average_loss, \"learning_rate\": self.scheduler.get_last_lr()[0]})\n            #print(f'Epoch {epoch+1}/{wandb.config[\"epochs\"]} - Average Loss: {average_loss:.4f} - Average accuracy {average_accuracy:.4f}')\n            print(f'Epoch {epoch+1}/{wandb.config[\"epochs\"]} - Average Loss: {average_loss:.4f} - Learning Rate {self.scheduler.get_last_lr()[0]}')\n\n        print(\"Training complete!\")\n\n    def validate(self, dataset: CustomDataset) -> tp.Dict[str, tp.Iterable]:\n        \"\"\"\n        takes a trained model and runs it on validation data.\n        Returns a dict with the keys \"valid_labels\" and \"valid_preds\" and corresponding values.\n        \"\"\"\n        \n        dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n        \n        self.model.to(self.device)\n        self.model.eval()\n        \n        valid_preds, valid_labels = [], []\n\n        for batch in dataloader:\n\n            b_input_ids = batch[\"input_ids\"].to(self.device)\n            b_input_mask = batch[\"attention_mask\"].to(self.device)\n            b_labels = batch[\"label\"].to(self.device)\n\n            with torch.no_grad():\n                logits = self.model(input_ids=b_input_ids, attention_mask=b_input_mask)\n\n            logits = logits[0].detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            batch_preds = np.argmax(logits, axis=1)\n            batch_labels = np.concatenate(label_ids.reshape(-1,1))\n            valid_preds.extend(batch_preds)\n            valid_labels.extend(batch_labels)\n\n        return valid_labels, valid_preds\n\n    def compute_metrics_report(\n        self, labels: tp.Iterable, predictions: tp.Iterable\n    ) -> tp.Any:\n        \"\"\"\n        Computes classification metric (or several metrcis) for given task.\n        \"\"\"\n        \n        recall = recall_score(labels, predictions)\n        precision = precision_score(labels, predictions)\n        f1 = f1_score(labels, predictions)\n\n        results = {\n            'recall_score': recall,\n            'precision_score': precision,\n            'f1_score': f1\n        }\n        return results\n\n    def save_model(self, dst_path: str) -> None:\n        \"\"\"\n        Saves model to dst_path. Be careful to check if a model is on DataParallel state.\n        If it is, one needs to process it accordingly.\n        \"\"\"\n        if isinstance(self.model, torch.nn.DataParallel):\n            torch.save(model.module.state_dict(), dst_path)\n        else:\n            torch.save(self.model, dst_path)\n\n    def run_experiment(self):\n        \"\"\"\n        Main entrypoint.\n        Runs the flow from loading data to computing metrics.\n        \"\"\"\n        wandb.login(key = 'ed5e812f0a2ec095a0e7b29e696ac3f9655e62ed')\n        \n          # Соберем общий конфиг для wandb\n        wandb_config = {\n            \"epochs\": 5, \n            \"lr\": 5e-2, \n            \"weight_decay\": 5e-4, \n            \"learning_rate_decay_step\": 2, \n            \"learning_rate_decay_factor\": 0.9,\n            \"sample_size\": self.sample_size,\n            \"batch_size\": 16\n            ,}\n        \n        # Init для wandb\n        wandb.init(project=\"hw-07-transformers\", \n                   notes=\"Dataset Name \" + self.dataset_name, \n                   tags=[\"baseline\", \"transformer\", \"distilbert-base\"], \n                   config=wandb_config,\n                  )\n        \n        train_df = self.load_data(filename = self.dataset_name, split = 'train', sample_size = self.sample_size)\n   \n        train_dataset = CustomDataset(\n            texts=train_df[\"text\"].tolist(),\n            labels=train_df[\"label\"].tolist(),\n            tokenizer=self.tokenizer,\n            max_length=512,\n            \n        )\n      \n        # Для оптимайзера - срез из конфига по wandb        \n        optimizer_param_list = ['lr', 'weight_decay']\n        \n        optimizer_params = {k: wandb_config[k] for k in optimizer_param_list}\n        self.configure_optimizer(optimizer = AdamW(self.model.parameters()), params = optimizer_params)\n        \n        # Выбираем экспоненциальный шедулер\n        self.configure_scheduler(scheduler = ExponentialLR(optimizer=self.optimizer, gamma=wandb.config[\"learning_rate_decay_factor\"], last_epoch=-1))\n        \n        self.apply_data_parallel()\n        \n        self.train(train_dataset)\n        \n        test_df = self.load_data(filename = self.dataset_name, split = 'test', sample_size = self.sample_size)\n        \n        test_dataset = CustomDataset(\n            texts=test_df[\"text\"].tolist(),\n            labels=test_df[\"label\"].tolist(),\n            tokenizer=self.tokenizer,\n            max_length=512,\n        )\n        \n        \n        valid_labels, valid_preds = self.validate(test_dataset)\n        \n        valid_scores = self.compute_metrics_report(valid_labels, valid_preds)\n        \n        self.save_model(dst_path = './distilbert-base-uncased_baseline.pt')\n        \n        pprint(valid_scores)\n        \n\nif __name__ == \"__main__\":\n    \"\"\"run experiment\"\"\"\n    model_trainer = ModelTrainer(model_name = \"distilbert-base-uncased\",\n                                 dataset_name = \"imdb\",\n                                 sample_size = 5000,\n                                )\n    model_trainer.run_experiment()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T11:21:29.240120Z","iopub.execute_input":"2024-05-20T11:21:29.240685Z","iopub.status.idle":"2024-05-20T11:34:51.259785Z","shell.execute_reply.started":"2024-05-20T11:21:29.240651Z","shell.execute_reply":"2024-05-20T11:34:51.258744Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:db5fpso1) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>██▄▄▁</td></tr><tr><td>loss</td><td>█▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.03645</td></tr><tr><td>loss</td><td>0.69615</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">elated-oath-43</strong> at: <a href='https://wandb.ai/an4sim/hw-07-transformers/runs/db5fpso1' target=\"_blank\">https://wandb.ai/an4sim/hw-07-transformers/runs/db5fpso1</a><br/> View project at: <a href='https://wandb.ai/an4sim/hw-07-transformers' target=\"_blank\">https://wandb.ai/an4sim/hw-07-transformers</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240520_103535-db5fpso1/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:db5fpso1). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240520_112129-kv59iaxr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/an4sim/hw-07-transformers/runs/kv59iaxr' target=\"_blank\">northern-glade-44</a></strong> to <a href='https://wandb.ai/an4sim/hw-07-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/an4sim/hw-07-transformers' target=\"_blank\">https://wandb.ai/an4sim/hw-07-transformers</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/an4sim/hw-07-transformers/runs/kv59iaxr' target=\"_blank\">https://wandb.ai/an4sim/hw-07-transformers/runs/kv59iaxr</a>"},"metadata":{}},{"name":"stderr","text":" 20%|██        | 1/5 [02:23<09:32, 143.21s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 - Average Loss: 2.6151 - Learning Rate 0.045000000000000005\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 2/5 [04:46<07:09, 143.29s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 - Average Loss: 0.6962 - Learning Rate 0.045000000000000005\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 3/5 [07:09<04:46, 143.31s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 - Average Loss: 0.6963 - Learning Rate 0.04050000000000001\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 4/5 [09:33<02:23, 143.27s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 - Average Loss: 0.6962 - Learning Rate 0.04050000000000001\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5/5 [11:56<00:00, 143.27s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 - Average Loss: 0.6961 - Learning Rate 0.03645000000000001\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"{'f1_score': 0.663994655978624, 'precision_score': 0.497, 'recall_score': 1.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"ds = load_dataset(\"imdb\", split = \"train\")\ndf = ds.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:34:19.414883Z","iopub.execute_input":"2024-05-20T10:34:19.415695Z","iopub.status.idle":"2024-05-20T10:34:25.975507Z","shell.execute_reply.started":"2024-05-20T10:34:19.415656Z","shell.execute_reply":"2024-05-20T10:34:25.974665Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df[\"text\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-20T10:35:10.077425Z","iopub.execute_input":"2024-05-20T10:35:10.077782Z","iopub.status.idle":"2024-05-20T10:35:10.086036Z","shell.execute_reply.started":"2024-05-20T10:35:10.077754Z","shell.execute_reply":"2024-05-20T10:35:10.085021Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0        I rented I AM CURIOUS-YELLOW from my video sto...\n1        \"I Am Curious: Yellow\" is a risible and preten...\n2        If only to avoid making this type of film in t...\n3        This film was probably inspired by Godard's Ma...\n4        Oh, brother...after hearing about this ridicul...\n                               ...                        \n24995    A hit at the time but now better categorised a...\n24996    I love this movie like no other. Another time ...\n24997    This film and it's sequel Barry Mckenzie holds...\n24998    'The Adventures Of Barry McKenzie' started lif...\n24999    The story centers around Barry McKenzie who mu...\nName: text, Length: 25000, dtype: object"},"metadata":{}}]}]}