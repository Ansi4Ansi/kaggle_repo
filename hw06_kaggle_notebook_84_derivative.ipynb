{"metadata":{"colab":{"provenance":[],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### <a href=\"https://colab.research.google.com/github/Ansi4Ansi/Google_colab/blob/main/ml_course/ml_b2c2024q2_simkin_HW06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"#!pip install pytorchtools\nimport torch\nimport plotly.graph_objects as go\nimport numpy as np\nfrom torch import nn\nfrom collections import defaultdict\nimport torchvision\nfrom torchvision import transforms as T\nfrom torch.utils.data import DataLoader, ConcatDataset\nimport matplotlib.pyplot as plt\n\n#from pytorchtools import EarlyStopping\n\ntorch.manual_seed(42)","metadata":{"id":"JvzCFL4510GI","outputId":"779b0676-34ac-481a-e6cf-55ff1f7c4141","execution":{"iopub.status.busy":"2024-05-08T03:59:48.879539Z","iopub.execute_input":"2024-05-08T03:59:48.880234Z","iopub.status.idle":"2024-05-08T03:59:53.420349Z","shell.execute_reply.started":"2024-05-08T03:59:48.880195Z","shell.execute_reply":"2024-05-08T03:59:53.419208Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f2d0cbfda50>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Оформление ДЗ**:\n\n- Выполненное ДЗ сохраните в файл ``ml_b2c2024q2_<фамилия>_HW06.ipynb`` (пример ``ml_b2c2024q2_dral_HW05.ipynb``)\n- Зарегистрироваться и залогиниться в сервисе [Everest](https://everest.distcomp.org/)\n- Перейти на страницу приложения: [BDT-grader-ML-B2C](https://everest.distcomp.org/apps/BigDataTeam/BDT-grader-ML-B2C)\n- Выбрать вкладку Submit Job (если отображается иная).\n- Выбрать в качестве “Task” значение: ``HW06:Introduction to neural networks`` (кодовое название для преподвателей: ``nn_intro``)\n- Загрузить в качестве “Task solution” файл с решением\n- В качестве Access Token указать тот, который был выслан по почте или в телеграм от аккаунта @bdt_manager\n\n**Вопросы**:\n- Свои вопросы присылайте в Телеграм.\n\n**Фидбек**:\n- Пожалуйста, оставьте свой отзыв после выполнения домашнего задания по сссылке:\n\n    https://forms.gle/iY5NRn9UfaZ344rbA","metadata":{"id":"9xWNVPoMeGa_"}},{"cell_type":"markdown","source":"# Вопросы на понимание (10%)\n\n1. Какие меры две меры ошибки мы используем для регрессии и для классификации в нейросетях?\n2. В чем задача активаций в нейросетях? Что будет происходить если их не использовать?\n3. Как можно понять что при тренировке выбран слишком высокий learning rate? А слишком низкий?\n4. У нас есть конволюционный слой с 3х3 фильтром, 32 входными каналами и 128 выходными. Какова размерность весов этого слоя? Сколько всего тренируемых параметров в этом слое?\n5. Может ли нейросеть делать правильное предсказание БЕЗ обучения? Почему/почему нет?\n\nПишите ответы внизу.","metadata":{"id":"8072bu3wd9Ir"}},{"cell_type":"markdown","source":"4. \n    Количество весов фильтра: 3 * 3 * 32 = 324\\\n    Количество выходных каналов: 128\\\n    Количество тренируемых параметров: 324 * 128 = 41472\n\n","metadata":{"id":"qLnyaYwgd9Is"}},{"cell_type":"markdown","source":"# Простые операции на тензорах (5%)","metadata":{"id":"cE55eP0Qd9Is"}},{"cell_type":"code","source":"# создайте тензоры x,y нормально распределенных случайных чисел размером (3, 5, 4) и (4, 7)\n\nx = torch.rand(3, 5, 4)\ny = torch.rand(4 ,7)\n# проверка\nassert x.size() == (3, 5, 4)\nassert y.size() == (4, 7)","metadata":{"id":"fdVyh0sI2vtZ","execution":{"iopub.status.busy":"2024-05-08T03:35:55.372010Z","iopub.execute_input":"2024-05-08T03:35:55.372477Z","iopub.status.idle":"2024-05-08T03:35:55.392574Z","shell.execute_reply.started":"2024-05-08T03:35:55.372445Z","shell.execute_reply":"2024-05-08T03:35:55.391853Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# каков будет размер (измерения) произведения этих тензоров? (dot product) запишите ответ в переменную\n\ndot_product_shape = (3, 5, 7)\n\n# проверка\nassert (x @ y).size() == dot_product_shape","metadata":{"id":"XCHSVdgwmWGl","execution":{"iopub.status.busy":"2024-05-08T03:35:59.191002Z","iopub.execute_input":"2024-05-08T03:35:59.191352Z","iopub.status.idle":"2024-05-08T03:35:59.245551Z","shell.execute_reply.started":"2024-05-08T03:35:59.191323Z","shell.execute_reply":"2024-05-08T03:35:59.244598Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Если мы объединим 2 последних измерения тензора x в одно, каким будет размер тензора х?\n# Проверьте себя\n\nnew_x_shape = (3, 20)\nx_reshaped = x.reshape(new_x_shape)\nassert x_reshaped.size() == new_x_shape","metadata":{"id":"BpTTbgIb3coL","outputId":"ac8d380b-25c7-4c7f-e217-56de004ff9ac","execution":{"iopub.status.busy":"2024-05-08T03:36:02.879665Z","iopub.execute_input":"2024-05-08T03:36:02.880514Z","iopub.status.idle":"2024-05-08T03:36:02.885000Z","shell.execute_reply.started":"2024-05-08T03:36:02.880480Z","shell.execute_reply":"2024-05-08T03:36:02.884200Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Другие операции с тензорами (работает так же как numpy), работаем с тензором x\n\n# Максимальное значение в тенсоре\nprint(f\"Max value in x: {x.max()}\")\n\n# Среднее значение по второму измерению (hint: индексация с нуля)\nprint(f\"Mean value in dim 2:\\n{x.mean(axis=1)}\")","metadata":{"id":"IyDS7rpxmgN2","outputId":"7458ee08-9ad0-446c-c068-e6b3d11c933a","execution":{"iopub.status.busy":"2024-05-08T03:36:08.163537Z","iopub.execute_input":"2024-05-08T03:36:08.163882Z","iopub.status.idle":"2024-05-08T03:36:08.217651Z","shell.execute_reply.started":"2024-05-08T03:36:08.163853Z","shell.execute_reply":"2024-05-08T03:36:08.216725Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Max value in x: 0.9593056440353394\nMean value in dim 2:\ntensor([[0.7937, 0.5581, 0.5163, 0.6807],\n        [0.4772, 0.4408, 0.4999, 0.4515],\n        [0.5904, 0.4485, 0.6079, 0.5389]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Автоматическая дифференциация (10%)\n\nОдна из главных фишек PyTorch и других библиотек для Deep Learning заключается в очень быстром расчете производных и градиентов.\n\nСоздайте функцию `def f(x:torch.Tensor) -> torch.Tensor:` вида\n\n$x^4 - 3x^3 - 10x^2 - 3x + 50$\n\nПосчитайте градиенты этой функции на промежутке (-2, 8) с шагом 0.1 (или меньшим, для красоты)\n\nВоспользуйтесь функцией `plot_derivative_results` для построения графика","metadata":{"id":"pjAPRCqPpPH_"}},{"cell_type":"code","source":"# постройте график функции f(x) и ее градиента\ndef plot_derivative_results (x: torch.Tensor, y: torch.Tensor, derivative: torch.Tensor, analytic: torch.Tensor):    \n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name='$f(x)=x^4-3x^3-10x^2-50$', opacity=0.7))\n    fig.add_trace(go.Scatter(x=x, y=derivative, mode='lines', name=\"$f(x)'$\", opacity=0.7))\n    fig.add_trace(go.Scatter(x=x, y=analytic, mode='lines', name=\"analytic\", opacity=0.7))\n    fig.update_layout(title='Function and grad', width=800, height=600, xaxis_title='x', yaxis_title=\"f(x), f(x)'\")\n    fig.show()\n    \ndef f(x: torch.Tensor) -> torch.Tensor:\n    return x**4 - 3*x**3 - 10*x*x -3*x + 50\n\ndef analytic_derivative(x: torch.Tensor) -> torch.Tensor:\n    return 4*x**3 - 9*x**2 - 20*x - 3\n\nx = torch.arange(-2, 8, 0.01, requires_grad = True)\ny = f(x)\nanalytic = analytic_derivative(x)\ngrads = torch.gradient(y, spacing = 0.01)\n\ndiff = abs(grads[0] - analytic)\n\nplot_derivative_results(x.detach(), y.detach(), grads[0].detach(), analytic.detach())\n\n\n\n","metadata":{"id":"uf8uqQUUrT6u","outputId":"d1e1529d-cd8f-42c6-87f7-f7bb701eb656","execution":{"iopub.status.busy":"2024-05-08T04:48:42.324151Z","iopub.execute_input":"2024-05-08T04:48:42.325086Z","iopub.status.idle":"2024-05-08T04:48:42.346455Z","shell.execute_reply.started":"2024-05-08T04:48:42.325048Z","shell.execute_reply":"2024-05-08T04:48:42.345678Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"9b186108-9a16-45a2-a3a9-ef3670db1470\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9b186108-9a16-45a2-a3a9-ef3670db1470\")) {                    Plotly.newPlot(                        \"9b186108-9a16-45a2-a3a9-ef3670db1470\",                        [{\"mode\":\"lines\",\"name\":\"$f(x)=x^4-3x^3-10x^2-50$\",\"opacity\":0.7,\"x\":[-2.0,-1.99,-1.98,-1.97,-1.96,-1.95,-1.94,-1.93,-1.92,-1.91,-1.9,-1.89,-1.88,-1.87,-1.86,-1.8499999,-1.84,-1.83,-1.82,-1.8100001,-1.8000001,-1.7900001,-1.7800001,-1.77,-1.76,-1.75,-1.74,-1.73,-1.72,-1.71,-1.7,-1.6899999,-1.68,-1.67,-1.66,-1.65,-1.64,-1.63,-1.62,-1.6099999,-1.6,-1.59,-1.58,-1.57,-1.5600001,-1.5500001,-1.5400001,-1.53,-1.52,-1.51,-1.5,-1.49,-1.48,-1.47,-1.46,-1.4499999,-1.44,-1.4300001,-1.4200001,-1.4100001,-1.4000001,-1.3900001,-1.3800001,-1.37,-1.36,-1.35,-1.34,-1.33,-1.32,-1.3100001,-1.3000001,-1.29,-1.28,-1.27,-1.26,-1.25,-1.24,-1.23,-1.22,-1.2099999,-1.2,-1.19,-1.1800001,-1.1700001,-1.1600001,-1.1500001,-1.1400001,-1.13,-1.12,-1.11,-1.1,-1.09,-1.08,-1.07,-1.0600001,-1.05,-1.04,-1.03,-1.02,-1.01,-0.99999994,-0.98999995,-0.97999996,-0.96999997,-0.96,-0.95,-0.94,-0.93,-0.91999996,-0.90999997,-0.9,-0.89,-0.88,-0.87,-0.86,-0.85,-0.84,-0.83,-0.82,-0.81,-0.8,-0.79,-0.78000003,-0.77000004,-0.76,-0.75,-0.74,-0.73,-0.72,-0.71000004,-0.70000005,-0.69000006,-0.68,-0.67,-0.66,-0.65000004,-0.64,-0.63,-0.62,-0.61,-0.59999996,-0.59,-0.58,-0.57,-0.56,-0.55,-0.54,-0.53000003,-0.52,-0.51,-0.5,-0.49,-0.48,-0.47,-0.45999998,-0.45,-0.44,-0.42999998,-0.42,-0.41,-0.4,-0.39000002,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.29999998,-0.29,-0.28,-0.26999998,-0.26,-0.25,-0.24,-0.22999999,-0.22,-0.21,-0.19999999,-0.19,-0.17999999,-0.17,-0.16,-0.14999999,-0.14,-0.13,-0.12,-0.11,-0.099999994,-0.089999996,-0.08,-0.07,-0.06,-0.049999997,-0.04,-0.029999997,-0.019999998,-0.009999998,4.1633363e-17,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.089999996,0.1,0.11,0.12,0.13,0.14,0.14999999,0.16,0.17,0.17999999,0.19,0.2,0.21,0.22,0.22999999,0.24,0.25,0.26,0.26999998,0.28,0.29,0.29999998,0.31,0.32,0.32999998,0.34,0.35,0.35999998,0.37,0.38,0.39,0.4,0.41,0.42000002,0.43,0.44,0.45000002,0.46,0.47,0.48,0.48999998,0.5,0.51,0.52,0.53,0.53999996,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.65999997,0.66999996,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76000005,0.77000004,0.78000003,0.79,0.8,0.81,0.82,0.83,0.84000003,0.85,0.86,0.87,0.88,0.89,0.9,0.90999997,0.92,0.93,0.94,0.95,0.96,0.96999997,0.97999996,0.98999995,1.0,1.01,1.02,1.03,1.04,1.05,1.06,1.0699999,1.0799999,1.0899999,1.0999999,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.2700001,1.28,1.29,1.3,1.31,1.3199999,1.3299999,1.3399999,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.4300001,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.5100001,1.52,1.53,1.54,1.55,1.56,1.5699999,1.5799999,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.6700001,1.68,1.6899999,1.6999999,1.7099999,1.7199999,1.7299999,1.7399999,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.8199999,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.9100001,1.92,1.93,1.9399999,1.9499999,1.9599999,1.9699999,1.9799999,1.99,2.0,2.01,2.02,2.03,2.04,2.05,2.06,2.07,2.08,2.09,2.1,2.11,2.12,2.1299999,2.1399999,2.1499999,2.16,2.17,2.18,2.19,2.2,2.21,2.22,2.23,2.24,2.25,2.26,2.27,2.28,2.29,2.3,2.31,2.32,2.33,2.34,2.35,2.36,2.37,2.3799999,2.3899999,2.4,2.41,2.42,2.43,2.44,2.45,2.46,2.47,2.48,2.49,2.5,2.51,2.52,2.53,2.54,2.55,2.56,2.57,2.58,2.59,2.6,2.61,2.62,2.6299999,2.64,2.65,2.66,2.67,2.68,2.69,2.7,2.71,2.72,2.73,2.74,2.75,2.76,2.77,2.78,2.79,2.8,2.81,2.82,2.83,2.84,2.85,2.86,2.87,2.88,2.89,2.9,2.91,2.92,2.93,2.94,2.95,2.96,2.97,2.98,2.99,3.0,3.01,3.02,3.03,3.04,3.05,3.06,3.07,3.08,3.09,3.1,3.11,3.12,3.1299999,3.1399999,3.1499999,3.1599998,3.1699998,3.1799998,3.1899998,3.2,3.21,3.22,3.23,3.24,3.25,3.26,3.27,3.28,3.29,3.3,3.31,3.32,3.33,3.34,3.35,3.36,3.37,3.3799999,3.3899999,3.3999999,3.4099998,3.4199998,3.4299998,3.44,3.45,3.46,3.47,3.48,3.49,3.5,3.51,3.52,3.53,3.54,3.55,3.56,3.57,3.58,3.59,3.6,3.61,3.62,3.6299999,3.6399999,3.6499999,3.6599998,3.6699998,3.68,3.69,3.7,3.71,3.72,3.73,3.74,3.75,3.76,3.77,3.78,3.79,3.8,3.81,3.82,3.83,3.84,3.85,3.86,3.87,3.8799999,3.8899999,3.8999999,3.9099998,3.92,3.93,3.94,3.95,3.96,3.97,3.98,3.99,4.0,4.01,4.02,4.03,4.04,4.05,4.06,4.07,4.08,4.09,4.1,4.11,4.12,4.13,4.14,4.15,4.16,4.17,4.18,4.19,4.2,4.21,4.22,4.23,4.24,4.25,4.2599998,4.27,4.2799997,4.29,4.2999997,4.31,4.32,4.3300004,4.34,4.3500004,4.36,4.3700004,4.38,4.3900003,4.4,4.4100003,4.42,4.4300003,4.44,4.4500003,4.46,4.4700003,4.48,4.4900002,4.5,4.51,4.52,4.53,4.54,4.55,4.56,4.57,4.58,4.59,4.6,4.61,4.62,4.63,4.64,4.65,4.66,4.67,4.68,4.69,4.7,4.71,4.72,4.73,4.74,4.75,4.7599998,4.77,4.7799997,4.79,4.8,4.8100004,4.82,4.8300004,4.84,4.8500004,4.86,4.8700004,4.88,4.8900003,4.9,4.9100003,4.92,4.9300003,4.94,4.9500003,4.96,4.9700003,4.98,4.9900002,5.0,5.01,5.02,5.03,5.04,5.05,5.06,5.07,5.08,5.09,5.1,5.11,5.12,5.13,5.14,5.15,5.16,5.17,5.18,5.19,5.2,5.21,5.22,5.23,5.24,5.25,5.2599998,5.27,5.28,5.2900004,5.3,5.3100004,5.32,5.3300004,5.34,5.3500004,5.36,5.3700004,5.38,5.3900003,5.4,5.4100003,5.42,5.4300003,5.44,5.4500003,5.46,5.4700003,5.48,5.4900002,5.5,5.51,5.52,5.53,5.54,5.55,5.56,5.57,5.58,5.59,5.6,5.61,5.62,5.63,5.64,5.65,5.66,5.67,5.68,5.69,5.7,5.71,5.72,5.73,5.74,5.75,5.76,5.7700005,5.78,5.7900004,5.8,5.8100004,5.82,5.8300004,5.84,5.8500004,5.86,5.8700004,5.88,5.8900003,5.9,5.9100003,5.92,5.9300003,5.94,5.9500003,5.96,5.9700003,5.98,5.9900002,6.0,6.01,6.02,6.03,6.04,6.05,6.06,6.07,6.08,6.09,6.1,6.11,6.12,6.13,6.14,6.15,6.16,6.17,6.18,6.19,6.2,6.21,6.22,6.23,6.24,6.25,6.2599998,6.27,6.2799997,6.29,6.2999997,6.31,6.32,6.3300004,6.34,6.3500004,6.36,6.3700004,6.38,6.3900003,6.4,6.4100003,6.42,6.4300003,6.44,6.4500003,6.46,6.4700003,6.48,6.4900002,6.5,6.51,6.52,6.53,6.54,6.55,6.56,6.57,6.58,6.59,6.6,6.61,6.62,6.63,6.64,6.65,6.66,6.67,6.68,6.69,6.7,6.71,6.72,6.73,6.74,6.75,6.7599998,6.77,6.7799997,6.79,6.8,6.8100004,6.82,6.8300004,6.84,6.8500004,6.86,6.8700004,6.88,6.8900003,6.9,6.9100003,6.92,6.9300003,6.94,6.9500003,6.96,6.9700003,6.98,6.9900002,7.0,7.01,7.02,7.03,7.04,7.05,7.06,7.07,7.08,7.09,7.1,7.11,7.12,7.13,7.14,7.15,7.16,7.17,7.18,7.19,7.2,7.21,7.22,7.23,7.24,7.25,7.2599998,7.27,7.28,7.2900004,7.3,7.3100004,7.32,7.3300004,7.34,7.3500004,7.36,7.3700004,7.38,7.3900003,7.4,7.4100003,7.42,7.4300003,7.44,7.4500003,7.46,7.4700003,7.48,7.4900002,7.5,7.51,7.52,7.53,7.54,7.55,7.56,7.57,7.58,7.59,7.6,7.61,7.62,7.63,7.64,7.65,7.66,7.67,7.68,7.69,7.7,7.71,7.72,7.73,7.74,7.75,7.76,7.7700005,7.78,7.7900004,7.8,7.8100004,7.82,7.8300004,7.84,7.8500004,7.86,7.8700004,7.88,7.8900003,7.9,7.9100003,7.92,7.93,7.94,7.95,7.96,7.97,7.98,7.99],\"y\":[56.0,55.69319,55.392715,55.098503,54.8105,54.528633,54.252834,53.98305,53.719204,53.461246,53.209103,52.962708,52.722,52.486916,52.2574,52.03338,51.8148,51.601593,51.3937,51.191055,50.993603,50.801273,50.614014,50.43176,50.25445,50.08203,49.914433,49.751602,49.593475,49.439995,49.2911,49.14673,49.00684,48.87135,48.74022,48.61338,48.49078,48.37236,48.258057,48.147827,48.0416,47.939323,47.84095,47.74641,47.65566,47.56863,47.48528,47.405544,47.329372,47.256706,47.1875,47.12169,47.059227,47.000057,46.944126,46.89138,46.84177,46.795235,46.75173,46.711205,46.673603,46.638866,46.606956,46.577812,46.551388,46.527634,46.50649,46.48792,46.471863,46.458275,46.4471,46.438297,46.431812,46.427593,46.425602,46.42578,46.428085,46.43247,46.438877,46.447273,46.4576,46.46982,46.483875,46.499725,46.517326,46.53663,46.55759,46.580166,46.6043,46.629963,46.6571,46.68567,46.715626,46.746925,46.779526,46.81338,46.84845,46.88469,46.92206,46.960506,47.0,47.040493,47.081944,47.124313,47.167553,47.21163,47.2565,47.302124,47.348457,47.395462,47.4431,47.49133,47.54011,47.58941,47.639175,47.68938,47.739983,47.790943,47.842224,47.89379,47.9456,47.99762,48.049805,48.10213,48.15455,48.20703,48.259537,48.312035,48.364483,48.41685,48.4691,48.5212,48.57311,48.6248,48.676235,48.727383,48.778206,48.82867,48.878746,48.928402,48.9776,49.02631,49.0745,49.12214,49.169193,49.21563,49.26142,49.306534,49.35094,49.394604,49.4375,49.479595,49.52086,49.561264,49.600784,49.63938,49.677032,49.71371,49.749382,49.78402,49.8176,49.85009,49.881466,49.9117,49.940765,49.96863,49.995274,50.02067,50.04479,50.067608,50.0891,50.10924,50.128002,50.145363,50.161297,50.17578,50.18879,50.2003,50.210285,50.218727,50.2256,50.23088,50.234547,50.236576,50.236942,50.23563,50.232616,50.227875,50.22139,50.21314,50.2031,50.191254,50.177578,50.162052,50.14466,50.12538,50.104195,50.08108,50.056023,50.029003,50.0,49.968998,49.935978,49.90092,49.86381,49.82463,49.783363,49.739994,49.694504,49.646877,49.5971,49.545155,49.491024,49.434696,49.376152,49.31538,49.25237,49.187096,49.119553,49.049725,48.9776,48.90316,48.826397,48.7473,48.665848,48.58203,48.495842,48.407265,48.31629,48.222904,48.1271,48.028862,47.92818,47.82505,47.71945,47.61138,47.500828,47.387783,47.272236,47.15418,47.0336,46.910496,46.78485,46.656666,46.52593,46.39263,46.256767,46.11833,45.97731,45.833702,45.6875,45.5387,45.38729,45.233276,45.076637,44.91738,44.755497,44.59098,44.42383,44.254036,44.0816,43.906517,43.72878,43.54839,43.36534,43.17963,42.99126,42.800224,42.606518,42.410145,42.2111,42.009384,41.804993,41.59793,41.38819,41.17578,40.960693,40.74293,40.52249,40.299385,40.0736,39.845146,39.614017,39.380222,39.14376,38.904633,38.66284,38.41839,38.17128,37.921516,37.6691,37.41404,37.15633,36.89598,36.632996,36.367382,36.09914,35.828274,35.554794,35.2787,35.0,34.7187,34.434807,34.14833,33.85927,33.567635,33.27343,32.97667,32.677353,32.375496,32.071106,31.764175,31.454735,31.142782,30.82833,30.511383,30.191954,29.870049,29.545685,29.21886,28.889599,28.557903,28.22379,27.887264,27.548342,27.207031,26.863344,26.517296,26.1689,25.818165,25.465101,25.109726,24.752058,24.3921,24.02987,23.665379,23.29865,22.929695,22.558523,22.185156,21.809597,21.43188,21.052006,20.66999,20.285864,19.89963,19.511307,19.120918,18.728476,18.333996,17.9375,17.538998,17.138523,16.736084,16.331696,15.9253845,15.517162,15.107056,14.695084,14.28125,13.865601,13.448135,13.028889,12.60788,12.18512,11.760632,11.334442,10.90657,10.477047,10.045883,9.613102,9.17873,8.74279,8.305302,7.866291,7.4257812,6.983799,6.5403633,6.0955048,5.6492424,5.201599,4.7526093,4.3022957,3.850666,3.3977776,2.9436302,2.488266,2.0317001,1.5739708,1.1150894,0.6551018,0.19401932,-0.268116,-0.7312889,-1.1954651,-1.660614,-2.1267166,-2.593727,-3.061634,-3.5304031,-4.0,-4.470394,-4.9415627,-5.4134636,-5.886074,-6.3593674,-6.8333054,-7.3078537,-7.782997,-8.258686,-8.734898,-9.211594,-9.688751,-10.166325,-10.644287,-11.122616,-11.601265,-12.080208,-12.559395,-13.038811,-13.518402,-13.998154,-14.478027,-14.95797,-15.437958,-15.917969,-16.39795,-16.877869,-17.357697,-17.83738,-18.316902,-18.796204,-19.275269,-19.754051,-20.232513,-20.710617,-21.188324,-21.665588,-22.14238,-22.618668,-23.094398,-23.569542,-24.044044,-24.517883,-24.991005,-25.463371,-25.934944,-26.4057,-26.875557,-27.344505,-27.8125,-28.279495,-28.745445,-29.210312,-29.67405,-30.13662,-30.597977,-31.058067,-31.516869,-31.974327,-32.430405,-32.88504,-33.338196,-33.789825,-34.239914,-34.68837,-35.135185,-35.580276,-36.02363,-36.465187,-36.9049,-37.342728,-37.778625,-38.212532,-38.64441,-39.07422,-39.501907,-39.927406,-40.350677,-40.771706,-41.190407,-41.60672,-42.02063,-42.432083,-42.841003,-43.24736,-43.651108,-44.052185,-44.450546,-44.84613,-45.238907,-45.628807,-46.015778,-46.399773,-46.78073,-47.158615,-47.53337,-47.904938,-48.27327,-48.638306,-49.0,-49.358284,-49.71312,-50.064453,-50.412224,-50.75638,-51.096832,-51.433586,-51.766556,-52.095665,-52.4209,-52.742165,-53.059433,-53.372635,-53.681717,-53.98661,-54.287262,-54.58364,-54.87567,-55.16326,-55.446396,-55.72499,-55.99904,-56.268394,-56.533066,-56.79297,-57.048035,-57.29824,-57.543488,-57.783737,-58.018898,-58.24894,-58.47381,-58.693398,-58.907692,-59.116623,-59.32009,-59.51808,-59.710503,-59.897285,-60.078407,-60.25374,-60.423294,-60.58696,-60.74466,-60.896362,-61.041992,-61.181503,-61.31478,-61.44182,-61.5625,-61.67678,-61.784615,-61.885895,-61.980606,-62.06862,-62.149895,-62.224396,-62.29203,-62.3527,-62.406403,-62.453003,-62.492477,-62.52475,-62.549713,-62.567368,-62.57759,-62.580315,-62.575493,-62.563057,-62.542915,-62.51496,-62.479233,-62.435577,-62.383926,-62.32422,-62.25638,-62.180374,-62.096085,-62.003456,-61.902397,-61.79287,-61.674767,-61.548042,-61.41259,-61.26837,-61.11528,-60.953262,-60.782257,-60.602158,-60.412888,-60.2144,-60.006615,-59.789436,-59.562782,-59.326607,-59.080833,-58.825363,-58.56009,-58.28502,-58.0,-57.704987,-57.399902,-57.084656,-56.759163,-56.423363,-56.077187,-55.720497,-55.35331,-54.97545,-54.586914,-54.187546,-53.777344,-53.35614,-52.92395,-52.48059,-52.026108,-51.560295,-51.083153,-50.594536,-50.094414,-49.582634,-49.059265,-48.524017,-47.977005,-47.41797,-46.84697,-46.263824,-45.668472,-45.060905,-44.44094,-43.80848,-43.163506,-42.50592,-41.83568,-41.152603,-40.45668,-39.747757,-39.025833,-38.290695,-37.54239,-36.780754,-36.00573,-35.217186,-34.415154,-33.599327,-32.769844,-31.926498,-31.069199,-30.197906,-29.3125,-28.412857,-27.498978,-26.570694,-25.627953,-24.670616,-23.698662,-22.711906,-21.71038,-20.693909,-19.662415,-18.615784,-17.553986,-16.476822,-15.384354,-14.276382,-13.152805,-12.013588,-10.858604,-9.687752,-8.500938,-7.2980156,-6.079052,-4.8437805,-3.59227,-2.3242188,-1.0397186,0.26145935,1.5792656,2.9139671,4.2656326,5.6342087,7.019951,8.422871,9.843037,11.280685,12.735733,14.208511,15.698822,17.207088,18.733135,20.2772,21.83935,23.419762,25.018318,26.635397,28.270939,29.925053,31.597902,33.289505,35.0,36.7296,38.47812,40.245968,42.033062,43.8397,45.665672,47.511375,49.37673,51.262012,53.167133,55.09233,57.03752,59.00316,60.989,62.995403,65.02223,67.06989,69.13816,71.227425,73.33753,75.46893,77.62128,79.795166,81.990265,84.20703,86.44534,88.705444,90.9874,93.29128,95.61715,97.96526,100.33556,102.72838,105.14357,107.5815,110.04201,112.525406,115.03169,117.56114,120.1136,122.68955,125.28861,127.911385,130.55759,133.22775,135.92151,138.63937,141.3812,144.14728,146.9375,149.75235,152.5914,155.45538,158.3439,161.25742,164.19586,167.1594,170.14804,173.16212,176.20149,179.26668,182.35732,185.47386,188.61623,191.78465,194.97908,198.20009,201.4471,204.7208,208.02106,211.34818,214.70187,218.0828,221.4906,224.92578,228.3883,231.87827,235.39563,238.94083,242.5137,246.11465,249.7434,253.40062,257.08582,260.7998,264.542,268.313,272.1128,275.9416,279.7992,283.68604,287.60217,291.54776,295.52277,299.52747,303.5619,307.62625,311.7207,315.8453,320.0,324.18536,328.401,332.64746,336.9246,341.2327,345.5718,349.9422,354.34357,358.77673,363.241,367.73737,372.26532,376.82538,381.41714,386.0414,390.69772,395.38687,400.10828,404.8625,409.64948,414.4696,419.32272,424.20908,429.12875,434.08203,439.06876,444.0894,449.14374,454.23245,459.35495,464.51205,469.70352,474.9296,480.19,485.4856,490.8159,496.18152,501.58197,507.01822,512.4896,517.99695,523.5399,529.1189,534.7335,540.38477,546.0721,551.796,557.55634,563.35376,569.1875,575.0586,580.96674,586.9122,592.8949,598.91547,604.9734,611.0694,617.20325,623.37537,629.58563,635.8344,642.1214,648.44745,654.81195,661.21576,667.6584,674.14044,680.66156,687.2226,693.8229,700.4635,707.14374,713.86414,720.6246,727.4258,734.2671,741.1496,748.07245,755.0364,762.0419,769.0882,776.1757,783.3052,790.476,797.68896,804.9436,812.24054,819.5796,826.96136,834.3852,841.8523,849.3617,856.9146,864.5102,872.14954,879.83203,887.55853,895.3282,903.14233,911.0,918.9023,926.8487,934.8396,942.8751,950.9558,959.08093,967.2518,975.46704,983.7285,992.0351,1000.3875,1008.7859,1017.2302,1025.7205,1034.2576,1042.8406,1051.4707,1060.1472,1068.8708,1077.6415,1086.4596,1095.3248,1104.2379,1113.1985,1122.207,1131.2634,1140.3682,1149.5217,1158.7234,1167.9734,1177.2723,1186.6204,1196.0177,1205.4636,1214.9597,1224.505,1234.1003,1243.7451,1253.4403,1263.1855,1272.9817,1282.8279,1292.7251,1302.6727,1312.6721,1322.7217,1332.8236,1342.9763,1353.1814,1363.4375,1373.7463,1384.1067,1394.5199,1404.9851,1415.5037,1426.0742,1436.6984,1447.3752,1458.106,1468.8895,1479.7273,1490.6183,1501.5641,1512.5626,1523.617,1534.7245,1545.8875,1557.1047,1568.3776,1579.705,1591.0883,1602.5262,1614.0203,1625.5698,1637.1758,1648.8379,1660.5564,1672.3306,1684.162,1696.0496,1707.9951,1719.997,1732.0566,1744.1731,1756.3479,1768.5801,1780.8707,1793.2191,1805.6263,1818.0913,1830.6155,1843.198,1855.8396,1868.5409,1881.3014,1894.1215,1907.0006,1919.9407,1932.94],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"$f(x)'$\",\"opacity\":0.7,\"x\":[-2.0,-1.99,-1.98,-1.97,-1.96,-1.95,-1.94,-1.93,-1.92,-1.91,-1.9,-1.89,-1.88,-1.87,-1.86,-1.8499999,-1.84,-1.83,-1.82,-1.8100001,-1.8000001,-1.7900001,-1.7800001,-1.77,-1.76,-1.75,-1.74,-1.73,-1.72,-1.71,-1.7,-1.6899999,-1.68,-1.67,-1.66,-1.65,-1.64,-1.63,-1.62,-1.6099999,-1.6,-1.59,-1.58,-1.57,-1.5600001,-1.5500001,-1.5400001,-1.53,-1.52,-1.51,-1.5,-1.49,-1.48,-1.47,-1.46,-1.4499999,-1.44,-1.4300001,-1.4200001,-1.4100001,-1.4000001,-1.3900001,-1.3800001,-1.37,-1.36,-1.35,-1.34,-1.33,-1.32,-1.3100001,-1.3000001,-1.29,-1.28,-1.27,-1.26,-1.25,-1.24,-1.23,-1.22,-1.2099999,-1.2,-1.19,-1.1800001,-1.1700001,-1.1600001,-1.1500001,-1.1400001,-1.13,-1.12,-1.11,-1.1,-1.09,-1.08,-1.07,-1.0600001,-1.05,-1.04,-1.03,-1.02,-1.01,-0.99999994,-0.98999995,-0.97999996,-0.96999997,-0.96,-0.95,-0.94,-0.93,-0.91999996,-0.90999997,-0.9,-0.89,-0.88,-0.87,-0.86,-0.85,-0.84,-0.83,-0.82,-0.81,-0.8,-0.79,-0.78000003,-0.77000004,-0.76,-0.75,-0.74,-0.73,-0.72,-0.71000004,-0.70000005,-0.69000006,-0.68,-0.67,-0.66,-0.65000004,-0.64,-0.63,-0.62,-0.61,-0.59999996,-0.59,-0.58,-0.57,-0.56,-0.55,-0.54,-0.53000003,-0.52,-0.51,-0.5,-0.49,-0.48,-0.47,-0.45999998,-0.45,-0.44,-0.42999998,-0.42,-0.41,-0.4,-0.39000002,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.29999998,-0.29,-0.28,-0.26999998,-0.26,-0.25,-0.24,-0.22999999,-0.22,-0.21,-0.19999999,-0.19,-0.17999999,-0.17,-0.16,-0.14999999,-0.14,-0.13,-0.12,-0.11,-0.099999994,-0.089999996,-0.08,-0.07,-0.06,-0.049999997,-0.04,-0.029999997,-0.019999998,-0.009999998,4.1633363e-17,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.089999996,0.1,0.11,0.12,0.13,0.14,0.14999999,0.16,0.17,0.17999999,0.19,0.2,0.21,0.22,0.22999999,0.24,0.25,0.26,0.26999998,0.28,0.29,0.29999998,0.31,0.32,0.32999998,0.34,0.35,0.35999998,0.37,0.38,0.39,0.4,0.41,0.42000002,0.43,0.44,0.45000002,0.46,0.47,0.48,0.48999998,0.5,0.51,0.52,0.53,0.53999996,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.65999997,0.66999996,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76000005,0.77000004,0.78000003,0.79,0.8,0.81,0.82,0.83,0.84000003,0.85,0.86,0.87,0.88,0.89,0.9,0.90999997,0.92,0.93,0.94,0.95,0.96,0.96999997,0.97999996,0.98999995,1.0,1.01,1.02,1.03,1.04,1.05,1.06,1.0699999,1.0799999,1.0899999,1.0999999,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.2700001,1.28,1.29,1.3,1.31,1.3199999,1.3299999,1.3399999,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.4300001,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.5100001,1.52,1.53,1.54,1.55,1.56,1.5699999,1.5799999,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.6700001,1.68,1.6899999,1.6999999,1.7099999,1.7199999,1.7299999,1.7399999,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.8199999,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.9100001,1.92,1.93,1.9399999,1.9499999,1.9599999,1.9699999,1.9799999,1.99,2.0,2.01,2.02,2.03,2.04,2.05,2.06,2.07,2.08,2.09,2.1,2.11,2.12,2.1299999,2.1399999,2.1499999,2.16,2.17,2.18,2.19,2.2,2.21,2.22,2.23,2.24,2.25,2.26,2.27,2.28,2.29,2.3,2.31,2.32,2.33,2.34,2.35,2.36,2.37,2.3799999,2.3899999,2.4,2.41,2.42,2.43,2.44,2.45,2.46,2.47,2.48,2.49,2.5,2.51,2.52,2.53,2.54,2.55,2.56,2.57,2.58,2.59,2.6,2.61,2.62,2.6299999,2.64,2.65,2.66,2.67,2.68,2.69,2.7,2.71,2.72,2.73,2.74,2.75,2.76,2.77,2.78,2.79,2.8,2.81,2.82,2.83,2.84,2.85,2.86,2.87,2.88,2.89,2.9,2.91,2.92,2.93,2.94,2.95,2.96,2.97,2.98,2.99,3.0,3.01,3.02,3.03,3.04,3.05,3.06,3.07,3.08,3.09,3.1,3.11,3.12,3.1299999,3.1399999,3.1499999,3.1599998,3.1699998,3.1799998,3.1899998,3.2,3.21,3.22,3.23,3.24,3.25,3.26,3.27,3.28,3.29,3.3,3.31,3.32,3.33,3.34,3.35,3.36,3.37,3.3799999,3.3899999,3.3999999,3.4099998,3.4199998,3.4299998,3.44,3.45,3.46,3.47,3.48,3.49,3.5,3.51,3.52,3.53,3.54,3.55,3.56,3.57,3.58,3.59,3.6,3.61,3.62,3.6299999,3.6399999,3.6499999,3.6599998,3.6699998,3.68,3.69,3.7,3.71,3.72,3.73,3.74,3.75,3.76,3.77,3.78,3.79,3.8,3.81,3.82,3.83,3.84,3.85,3.86,3.87,3.8799999,3.8899999,3.8999999,3.9099998,3.92,3.93,3.94,3.95,3.96,3.97,3.98,3.99,4.0,4.01,4.02,4.03,4.04,4.05,4.06,4.07,4.08,4.09,4.1,4.11,4.12,4.13,4.14,4.15,4.16,4.17,4.18,4.19,4.2,4.21,4.22,4.23,4.24,4.25,4.2599998,4.27,4.2799997,4.29,4.2999997,4.31,4.32,4.3300004,4.34,4.3500004,4.36,4.3700004,4.38,4.3900003,4.4,4.4100003,4.42,4.4300003,4.44,4.4500003,4.46,4.4700003,4.48,4.4900002,4.5,4.51,4.52,4.53,4.54,4.55,4.56,4.57,4.58,4.59,4.6,4.61,4.62,4.63,4.64,4.65,4.66,4.67,4.68,4.69,4.7,4.71,4.72,4.73,4.74,4.75,4.7599998,4.77,4.7799997,4.79,4.8,4.8100004,4.82,4.8300004,4.84,4.8500004,4.86,4.8700004,4.88,4.8900003,4.9,4.9100003,4.92,4.9300003,4.94,4.9500003,4.96,4.9700003,4.98,4.9900002,5.0,5.01,5.02,5.03,5.04,5.05,5.06,5.07,5.08,5.09,5.1,5.11,5.12,5.13,5.14,5.15,5.16,5.17,5.18,5.19,5.2,5.21,5.22,5.23,5.24,5.25,5.2599998,5.27,5.28,5.2900004,5.3,5.3100004,5.32,5.3300004,5.34,5.3500004,5.36,5.3700004,5.38,5.3900003,5.4,5.4100003,5.42,5.4300003,5.44,5.4500003,5.46,5.4700003,5.48,5.4900002,5.5,5.51,5.52,5.53,5.54,5.55,5.56,5.57,5.58,5.59,5.6,5.61,5.62,5.63,5.64,5.65,5.66,5.67,5.68,5.69,5.7,5.71,5.72,5.73,5.74,5.75,5.76,5.7700005,5.78,5.7900004,5.8,5.8100004,5.82,5.8300004,5.84,5.8500004,5.86,5.8700004,5.88,5.8900003,5.9,5.9100003,5.92,5.9300003,5.94,5.9500003,5.96,5.9700003,5.98,5.9900002,6.0,6.01,6.02,6.03,6.04,6.05,6.06,6.07,6.08,6.09,6.1,6.11,6.12,6.13,6.14,6.15,6.16,6.17,6.18,6.19,6.2,6.21,6.22,6.23,6.24,6.25,6.2599998,6.27,6.2799997,6.29,6.2999997,6.31,6.32,6.3300004,6.34,6.3500004,6.36,6.3700004,6.38,6.3900003,6.4,6.4100003,6.42,6.4300003,6.44,6.4500003,6.46,6.4700003,6.48,6.4900002,6.5,6.51,6.52,6.53,6.54,6.55,6.56,6.57,6.58,6.59,6.6,6.61,6.62,6.63,6.64,6.65,6.66,6.67,6.68,6.69,6.7,6.71,6.72,6.73,6.74,6.75,6.7599998,6.77,6.7799997,6.79,6.8,6.8100004,6.82,6.8300004,6.84,6.8500004,6.86,6.8700004,6.88,6.8900003,6.9,6.9100003,6.92,6.9300003,6.94,6.9500003,6.96,6.9700003,6.98,6.9900002,7.0,7.01,7.02,7.03,7.04,7.05,7.06,7.07,7.08,7.09,7.1,7.11,7.12,7.13,7.14,7.15,7.16,7.17,7.18,7.19,7.2,7.21,7.22,7.23,7.24,7.25,7.2599998,7.27,7.28,7.2900004,7.3,7.3100004,7.32,7.3300004,7.34,7.3500004,7.36,7.3700004,7.38,7.3900003,7.4,7.4100003,7.42,7.4300003,7.44,7.4500003,7.46,7.4700003,7.48,7.4900002,7.5,7.51,7.52,7.53,7.54,7.55,7.56,7.57,7.58,7.59,7.6,7.61,7.62,7.63,7.64,7.65,7.66,7.67,7.68,7.69,7.7,7.71,7.72,7.73,7.74,7.75,7.76,7.7700005,7.78,7.7900004,7.8,7.8100004,7.82,7.8300004,7.84,7.8500004,7.86,7.8700004,7.88,7.8900003,7.9,7.9100003,7.92,7.93,7.94,7.95,7.96,7.97,7.98,7.99],\"y\":[-30.680847,-30.364227,-29.73442,-29.110718,-28.4935,-27.883339,-27.27909,-26.681519,-26.09024,-25.505066,-24.926949,-24.355125,-23.789597,-23.22998,-22.67685,-22.130013,-21.58928,-21.05503,-20.526886,-20.004845,-19.489098,-18.979454,-18.475723,-17.978096,-17.486382,-17.000961,-16.521454,-16.04786,-15.580368,-15.11879,-14.663315,-14.21299,-13.768959,-13.331032,-12.898445,-12.471962,-12.05101,-11.636162,-11.226654,-10.822868,-10.425186,-10.032463,-9.645653,-9.2645645,-8.889008,-8.518982,-8.154297,-7.795334,-7.441902,-7.0936203,-6.7508698,-6.4136505,-6.081581,-5.755043,-5.4338455,-5.117798,-4.8072815,-4.501915,-4.2015076,-3.9064407,-3.6169052,-3.3323288,-3.0527115,-2.7784348,-2.5089264,-2.2449493,-1.9857407,-1.7313004,-1.4822006,-1.23806,-0.9988785,-0.76446533,-0.535202,-0.31051636,-0.09059906,0.124168396,0.33435822,0.5395889,0.740242,0.9361267,1.127243,1.3137817,1.4953613,1.672554,1.8451691,2.0132065,2.176857,2.3355484,2.489853,2.6399612,2.7853012,2.9262543,3.0628204,3.1949997,3.322792,3.4461975,3.5654068,3.68042,3.7908554,3.8970947,3.9993286,4.0971756,4.191017,4.280472,4.365921,4.447365,4.5246124,4.5978546,4.6669006,4.732132,4.793358,4.8505783,4.903984,4.9531937,4.9985886,5.0403595,5.078125,5.112076,5.1424026,5.168724,5.1914215,5.2103043,5.225563,5.237198,5.245018,5.249405,5.250168,5.247307,5.240822,5.2309036,5.2173615,5.200386,5.180168,5.1563263,5.129051,5.0985336,5.064392,5.027008,4.9865723,4.9427032,4.895401,4.845047,4.7914505,4.7346115,4.67453,4.611397,4.545212,4.475975,4.403496,4.327965,4.2495728,4.167938,4.0834427,3.9962769,3.9058685,3.8124084,3.7164688,3.6174774,3.5154343,3.4109116,3.3035278,3.193283,3.0805588,2.9649734,2.846527,2.7254105,2.602005,2.4757385,2.3468018,2.2155762,2.0816803,1.9451141,1.8060684,1.6647339,1.5209198,1.3746262,1.225853,1.074791,0.9214401,0.7658005,0.6076813,0.44727325,0.28476715,0.119781494,-0.047302246,-0.21629333,-0.38776398,-0.5613327,-0.7368088,-0.91457367,-1.0942459,-1.2760162,-1.4600754,-1.6458511,-1.8335342,-2.0233154,-2.215004,-2.4085999,-2.6039124,-2.8011322,-3.0002594,-3.2011032,-3.4038544,-3.6083221,-3.8145065,-4.0224075,-4.2318344,-4.442978,-4.655838,-4.870224,-5.086136,-5.3037643,-5.5229187,-5.743599,-5.965805,-6.1891556,-6.4142227,-6.6408157,-6.868553,-7.0976257,-7.328224,-7.560158,-7.793045,-8.027458,-8.263397,-8.50029,-8.738327,-8.977509,-9.218025,-9.459496,-9.70211,-9.94606,-10.190582,-10.4364395,-10.683441,-10.931206,-11.179924,-11.429596,-11.680222,-11.931801,-12.184143,-12.437439,-12.691498,-12.946129,-13.201714,-13.458061,-13.715172,-13.972855,-14.2313,-14.490509,-14.750099,-15.010452,-15.271187,-15.532684,-15.794754,-16.057014,-16.320038,-16.583443,-16.847229,-17.111397,-17.375946,-17.641068,-17.90638,-18.171883,-18.437958,-18.704033,-18.970299,-19.237137,-19.503975,-19.770813,-20.038033,-20.305443,-20.572662,-20.840073,-21.107483,-21.374893,-21.642494,-21.910095,-22.177315,-22.444534,-22.711945,-22.979164,-23.246193,-23.51284,-23.779488,-24.045944,-24.31221,-24.578094,-24.843597,-25.10891,-25.37384,-25.63858,-25.902939,-26.166725,-26.42994,-26.692772,-26.955414,-27.217293,-27.4786,-27.739716,-28.000069,-28.259659,-28.518486,-28.776932,-29.034805,-29.291916,-29.548264,-29.803848,-30.05867,-30.312347,-30.566025,-30.818558,-31.06966,-31.320286,-31.569958,-31.818771,-32.066727,-32.313442,-32.559395,-32.8043,-33.047867,-33.29048,-33.53195,-33.772373,-34.01165,-34.249878,-34.48677,-34.722233,-34.95655,-35.189915,-35.421944,-35.65216,-35.88133,-36.109447,-36.33604,-36.560917,-36.784172,-37.00638,-37.226963,-37.44631,-37.663746,-37.879562,-38.09452,-38.307095,-38.518047,-38.727856,-38.935566,-39.14156,-39.346123,-39.54878,-39.74991,-39.948845,-40.145683,-40.341377,-40.534973,-40.72666,-40.916443,-41.103935,-41.290283,-41.47415,-41.65573,-41.835594,-42.012787,-42.188454,-42.362404,-42.533875,-42.703056,-42.86976,-43.034363,-43.19725,-43.35766,-43.515587,-43.671417,-43.82496,-43.97602,-44.124603,-44.270897,-44.41471,-44.556046,-44.695282,-44.831657,-44.96517,-45.09716,-45.225906,-45.35179,-45.47558,-45.596504,-45.71476,-45.830536,-45.94345,-46.053505,-46.16089,-46.26541,-46.367455,-46.466255,-46.562576,-46.655655,-46.745872,-46.8338,-46.918297,-46.99955,-47.078133,-47.153473,-47.22557,-47.29519,-47.361565,-47.424316,-47.48459,-47.54162,-47.595024,-47.64538,-47.69268,-47.73655,-47.776794,-47.81456,-47.848892,-47.8796,-47.906494,-47.930145,-47.950363,-47.967148,-47.981262,-47.9908,-47.99652,-47.999954,-47.999573,-47.994995,-47.987366,-47.97554,-47.96028,-47.941208,-47.91832,-47.89238,-47.862244,-47.828293,-47.790527,-47.748566,-47.70279,-47.65396,-47.600937,-47.543716,-47.4823,-47.41707,-47.348022,-47.2744,-47.19696,-47.11647,-47.03064,-46.94023,-46.847153,-46.749496,-46.647263,-46.540833,-46.430206,-46.315384,-46.196365,-46.072388,-45.944595,-45.81299,-45.676804,-45.53566,-45.389557,-45.239258,-45.085907,-44.927216,-44.763565,-44.595337,-44.42215,-44.24553,-44.06357,-43.87703,-43.686295,-43.49022,-43.289185,-43.084335,-42.87491,-42.659378,-42.438507,-42.214966,-41.986465,-41.750717,-41.511154,-41.268158,-41.018677,-40.763855,-40.50522,-40.24124,-39.971924,-39.697266,-39.41803,-39.133835,-38.843536,-38.54828,-38.24768,-37.942123,-37.63199,-37.31613,-36.994934,-36.668396,-36.336517,-35.998917,-35.655975,-35.308456,-34.955215,-34.596252,-34.230423,-33.860397,-33.486176,-33.103943,-32.717133,-32.32498,-31.926727,-31.523514,-31.114197,-30.698776,-30.277252,-29.851532,-29.420471,-28.981018,-28.536224,-28.086472,-27.632141,-27.170181,-26.701355,-26.228714,-25.748444,-25.263596,-24.772644,-24.274826,-23.770523,-23.260117,-22.745514,-22.2229,-21.694183,-21.16127,-20.619965,-20.072937,-19.520569,-18.96019,-18.395233,-17.822647,-17.244339,-16.661072,-16.068268,-15.470123,-14.866638,-14.25705,-13.63945,-13.015747,-12.385941,-11.748123,-11.105728,-10.455704,-9.799576,-9.1362,-8.464432,-7.788849,-7.106781,-6.4151764,-5.7186127,-5.0151825,-4.3037415,-3.5873413,-2.861786,-2.13089,-1.3938904,-0.6473541,0.104904175,0.8628845,1.6288757,2.4047852,3.1841278,3.9691925,4.76532,5.567932,6.377411,7.19223,8.014679,8.8459015,9.684372,10.529327,11.381531,12.241364,13.108826,13.983536,14.865494,15.755463,16.651154,17.555237,18.468475,19.387817,20.313644,21.248245,22.19162,23.14148,24.097443,25.06218,26.037216,27.017212,28.004456,29.001617,30.004883,31.01654,32.036972,33.06465,34.098816,35.14328,36.193848,37.252426,38.31978,39.39514,40.478516,41.570282,42.669678,43.777466,44.89212,46.014786,47.14775,48.287964,49.43695,50.595093,51.75743,52.930832,54.113007,55.30243,56.50177,57.707214,58.924866,60.14595,61.37657,62.621307,63.871765,65.127945,66.39137,67.66586,68.94989,70.24231,71.54236,72.85309,74.17221,75.497055,76.83296,78.178406,79.52881,80.892944,82.26547,83.64143,85.03227,86.429596,87.83493,89.25247,90.67612,92.108154,93.551254,95.00389,96.46454,97.935486,99.41406,100.89989,102.3983,103.90625,105.42145,106.94809,108.4816,110.02197,111.577415,113.139725,114.710045,116.29181,117.8833,119.48681,121.09432,122.711754,124.3391,125.97809,127.62756,129.2839,130.94922,132.6254,134.31836,136.01208,137.71591,139.4331,141.1543,142.89075,144.63483,146.3913,148.15445,149.92886,151.71567,153.50562,155.31073,157.12805,158.94843,160.78177,162.63104,164.48279,166.34818,168.22263,170.10489,172.0047,173.90594,175.81845,177.74716,179.68655,181.63052,183.58383,185.55298,187.53186,189.52008,191.51593,193.51941,195.54138,197.57385,199.61224,201.66168,203.72449,205.79643,207.87659,209.96857,212.07542,214.18724,216.31165,218.4494,220.59326,222.7539,224.92065,227.10266,229.29192,231.48766,233.69904,235.92072,238.15575,240.40031,242.65594,244.92188,247.19543,249.48425,251.7868,254.09546,256.42053,258.75055,261.0916,263.44873,265.81842,268.1961,270.58105,272.9843,275.3952,277.81525,280.2536,282.695,285.15167,287.62512,290.10162,292.59796,295.09888,297.60895,300.13657,302.67258,305.22766,307.79114,310.3592,312.94556,315.53955,318.1427,320.7718,323.40088,326.0353,328.69797,331.36902,334.04083,336.73096,339.43634,342.14935,344.88525,347.6242,350.3662,353.12805,355.90363,358.6914,361.48453,364.29825,367.12112,369.95926,372.80884,375.65918,378.54004,381.43005,384.32007,387.2223,390.14893,393.08624,396.02966,398.9853,401.95618,404.93927,407.94067,410.95276,413.96484,417.00287,420.05005,423.10486,426.1795,429.26178,432.36084,435.47516,438.58795,441.72668,444.87152,448.03162,451.21613,454.40063,457.59125,460.8017,464.02893,467.27295,470.52765,473.78082,477.05994,480.35583,483.6624,486.97357,490.3015,493.64777,497.00012,500.36774,503.74908,507.15332,510.5606,513.9801,517.4286,520.8771,524.32404,527.8,531.29425,534.79614,538.30414,541.835,545.3827,548.93646,552.51465,556.0974,559.6802,563.29346,566.9281,570.56274,574.21265,577.88696,581.5582,585.2417,588.9618,592.67883,596.4081,600.1648,603.92456,607.69653,611.4929,615.29846,619.11926,622.9523,626.7883,630.65186,634.5276,638.4155,642.3218,646.23413,650.1587,654.10767,658.0658,662.0453,666.0431,670.03174,674.04175,678.0823,682.12585,686.1908,690.26794,694.339,698.47107,702.59094,706.6925,710.849,715.01465,719.18945,723.3795,727.57874,731.7993,736.04126,740.28015,744.5465,748.8251,753.11584,757.4249,761.7462,766.0919,770.4498,774.80774,779.19006,783.5907,787.99744,792.4347,796.86584,801.3214,805.81055,810.2905,814.798,819.3054,823.8373,828.4027,832.9498,837.5397,842.1356,846.7285,851.3672,856.00586,860.65674,865.33203,870.0073,874.71313,879.43726,884.1675,888.916,893.68286,898.4558,903.2471,908.05664,912.91504,917.7612,922.583,927.4475,932.34863,937.26807,942.1631,947.1008,952.0691,957.03125,962.0056,966.9983,972.0215,977.0691,982.1167,987.1704,992.24243,997.3511,1002.4475,1007.57446,1012.73193,1017.8894,1023.0591,1028.2471,1033.4595,1038.678,1043.9209,1049.1882,1054.4556,1059.7351,1065.0513,1070.3796,1075.7141,1081.0669,1086.438,1091.8396,1097.2168,1102.6428,1108.0933,1113.5254,1119.0125,1124.5056,1130.011,1135.5347,1141.0645,1146.6003,1152.179,1157.7759,1163.4033,1169.0308,1174.6338,1180.2795,1185.9497,1191.6565,1197.3694,1203.0762,1208.8074,1214.563,1220.3491,1226.1414,1231.9519,1237.7808,1243.6096,1249.4568,1255.3345,1261.206,1267.1448,1273.0896,1279.0283,1284.9609,1290.9607,1296.9666,1299.9268],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"analytic\",\"opacity\":0.7,\"x\":[-2.0,-1.99,-1.98,-1.97,-1.96,-1.95,-1.94,-1.93,-1.92,-1.91,-1.9,-1.89,-1.88,-1.87,-1.86,-1.8499999,-1.84,-1.83,-1.82,-1.8100001,-1.8000001,-1.7900001,-1.7800001,-1.77,-1.76,-1.75,-1.74,-1.73,-1.72,-1.71,-1.7,-1.6899999,-1.68,-1.67,-1.66,-1.65,-1.64,-1.63,-1.62,-1.6099999,-1.6,-1.59,-1.58,-1.57,-1.5600001,-1.5500001,-1.5400001,-1.53,-1.52,-1.51,-1.5,-1.49,-1.48,-1.47,-1.46,-1.4499999,-1.44,-1.4300001,-1.4200001,-1.4100001,-1.4000001,-1.3900001,-1.3800001,-1.37,-1.36,-1.35,-1.34,-1.33,-1.32,-1.3100001,-1.3000001,-1.29,-1.28,-1.27,-1.26,-1.25,-1.24,-1.23,-1.22,-1.2099999,-1.2,-1.19,-1.1800001,-1.1700001,-1.1600001,-1.1500001,-1.1400001,-1.13,-1.12,-1.11,-1.1,-1.09,-1.08,-1.07,-1.0600001,-1.05,-1.04,-1.03,-1.02,-1.01,-0.99999994,-0.98999995,-0.97999996,-0.96999997,-0.96,-0.95,-0.94,-0.93,-0.91999996,-0.90999997,-0.9,-0.89,-0.88,-0.87,-0.86,-0.85,-0.84,-0.83,-0.82,-0.81,-0.8,-0.79,-0.78000003,-0.77000004,-0.76,-0.75,-0.74,-0.73,-0.72,-0.71000004,-0.70000005,-0.69000006,-0.68,-0.67,-0.66,-0.65000004,-0.64,-0.63,-0.62,-0.61,-0.59999996,-0.59,-0.58,-0.57,-0.56,-0.55,-0.54,-0.53000003,-0.52,-0.51,-0.5,-0.49,-0.48,-0.47,-0.45999998,-0.45,-0.44,-0.42999998,-0.42,-0.41,-0.4,-0.39000002,-0.38,-0.37,-0.36,-0.35,-0.34,-0.33,-0.32,-0.31,-0.29999998,-0.29,-0.28,-0.26999998,-0.26,-0.25,-0.24,-0.22999999,-0.22,-0.21,-0.19999999,-0.19,-0.17999999,-0.17,-0.16,-0.14999999,-0.14,-0.13,-0.12,-0.11,-0.099999994,-0.089999996,-0.08,-0.07,-0.06,-0.049999997,-0.04,-0.029999997,-0.019999998,-0.009999998,4.1633363e-17,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.089999996,0.1,0.11,0.12,0.13,0.14,0.14999999,0.16,0.17,0.17999999,0.19,0.2,0.21,0.22,0.22999999,0.24,0.25,0.26,0.26999998,0.28,0.29,0.29999998,0.31,0.32,0.32999998,0.34,0.35,0.35999998,0.37,0.38,0.39,0.4,0.41,0.42000002,0.43,0.44,0.45000002,0.46,0.47,0.48,0.48999998,0.5,0.51,0.52,0.53,0.53999996,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65,0.65999997,0.66999996,0.68,0.69,0.7,0.71,0.72,0.73,0.74,0.75,0.76000005,0.77000004,0.78000003,0.79,0.8,0.81,0.82,0.83,0.84000003,0.85,0.86,0.87,0.88,0.89,0.9,0.90999997,0.92,0.93,0.94,0.95,0.96,0.96999997,0.97999996,0.98999995,1.0,1.01,1.02,1.03,1.04,1.05,1.06,1.0699999,1.0799999,1.0899999,1.0999999,1.11,1.12,1.13,1.14,1.15,1.16,1.17,1.18,1.19,1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.2700001,1.28,1.29,1.3,1.31,1.3199999,1.3299999,1.3399999,1.35,1.36,1.37,1.38,1.39,1.4,1.41,1.42,1.4300001,1.44,1.45,1.46,1.47,1.48,1.49,1.5,1.5100001,1.52,1.53,1.54,1.55,1.56,1.5699999,1.5799999,1.59,1.6,1.61,1.62,1.63,1.64,1.65,1.66,1.6700001,1.68,1.6899999,1.6999999,1.7099999,1.7199999,1.7299999,1.7399999,1.75,1.76,1.77,1.78,1.79,1.8,1.81,1.8199999,1.83,1.84,1.85,1.86,1.87,1.88,1.89,1.9,1.9100001,1.92,1.93,1.9399999,1.9499999,1.9599999,1.9699999,1.9799999,1.99,2.0,2.01,2.02,2.03,2.04,2.05,2.06,2.07,2.08,2.09,2.1,2.11,2.12,2.1299999,2.1399999,2.1499999,2.16,2.17,2.18,2.19,2.2,2.21,2.22,2.23,2.24,2.25,2.26,2.27,2.28,2.29,2.3,2.31,2.32,2.33,2.34,2.35,2.36,2.37,2.3799999,2.3899999,2.4,2.41,2.42,2.43,2.44,2.45,2.46,2.47,2.48,2.49,2.5,2.51,2.52,2.53,2.54,2.55,2.56,2.57,2.58,2.59,2.6,2.61,2.62,2.6299999,2.64,2.65,2.66,2.67,2.68,2.69,2.7,2.71,2.72,2.73,2.74,2.75,2.76,2.77,2.78,2.79,2.8,2.81,2.82,2.83,2.84,2.85,2.86,2.87,2.88,2.89,2.9,2.91,2.92,2.93,2.94,2.95,2.96,2.97,2.98,2.99,3.0,3.01,3.02,3.03,3.04,3.05,3.06,3.07,3.08,3.09,3.1,3.11,3.12,3.1299999,3.1399999,3.1499999,3.1599998,3.1699998,3.1799998,3.1899998,3.2,3.21,3.22,3.23,3.24,3.25,3.26,3.27,3.28,3.29,3.3,3.31,3.32,3.33,3.34,3.35,3.36,3.37,3.3799999,3.3899999,3.3999999,3.4099998,3.4199998,3.4299998,3.44,3.45,3.46,3.47,3.48,3.49,3.5,3.51,3.52,3.53,3.54,3.55,3.56,3.57,3.58,3.59,3.6,3.61,3.62,3.6299999,3.6399999,3.6499999,3.6599998,3.6699998,3.68,3.69,3.7,3.71,3.72,3.73,3.74,3.75,3.76,3.77,3.78,3.79,3.8,3.81,3.82,3.83,3.84,3.85,3.86,3.87,3.8799999,3.8899999,3.8999999,3.9099998,3.92,3.93,3.94,3.95,3.96,3.97,3.98,3.99,4.0,4.01,4.02,4.03,4.04,4.05,4.06,4.07,4.08,4.09,4.1,4.11,4.12,4.13,4.14,4.15,4.16,4.17,4.18,4.19,4.2,4.21,4.22,4.23,4.24,4.25,4.2599998,4.27,4.2799997,4.29,4.2999997,4.31,4.32,4.3300004,4.34,4.3500004,4.36,4.3700004,4.38,4.3900003,4.4,4.4100003,4.42,4.4300003,4.44,4.4500003,4.46,4.4700003,4.48,4.4900002,4.5,4.51,4.52,4.53,4.54,4.55,4.56,4.57,4.58,4.59,4.6,4.61,4.62,4.63,4.64,4.65,4.66,4.67,4.68,4.69,4.7,4.71,4.72,4.73,4.74,4.75,4.7599998,4.77,4.7799997,4.79,4.8,4.8100004,4.82,4.8300004,4.84,4.8500004,4.86,4.8700004,4.88,4.8900003,4.9,4.9100003,4.92,4.9300003,4.94,4.9500003,4.96,4.9700003,4.98,4.9900002,5.0,5.01,5.02,5.03,5.04,5.05,5.06,5.07,5.08,5.09,5.1,5.11,5.12,5.13,5.14,5.15,5.16,5.17,5.18,5.19,5.2,5.21,5.22,5.23,5.24,5.25,5.2599998,5.27,5.28,5.2900004,5.3,5.3100004,5.32,5.3300004,5.34,5.3500004,5.36,5.3700004,5.38,5.3900003,5.4,5.4100003,5.42,5.4300003,5.44,5.4500003,5.46,5.4700003,5.48,5.4900002,5.5,5.51,5.52,5.53,5.54,5.55,5.56,5.57,5.58,5.59,5.6,5.61,5.62,5.63,5.64,5.65,5.66,5.67,5.68,5.69,5.7,5.71,5.72,5.73,5.74,5.75,5.76,5.7700005,5.78,5.7900004,5.8,5.8100004,5.82,5.8300004,5.84,5.8500004,5.86,5.8700004,5.88,5.8900003,5.9,5.9100003,5.92,5.9300003,5.94,5.9500003,5.96,5.9700003,5.98,5.9900002,6.0,6.01,6.02,6.03,6.04,6.05,6.06,6.07,6.08,6.09,6.1,6.11,6.12,6.13,6.14,6.15,6.16,6.17,6.18,6.19,6.2,6.21,6.22,6.23,6.24,6.25,6.2599998,6.27,6.2799997,6.29,6.2999997,6.31,6.32,6.3300004,6.34,6.3500004,6.36,6.3700004,6.38,6.3900003,6.4,6.4100003,6.42,6.4300003,6.44,6.4500003,6.46,6.4700003,6.48,6.4900002,6.5,6.51,6.52,6.53,6.54,6.55,6.56,6.57,6.58,6.59,6.6,6.61,6.62,6.63,6.64,6.65,6.66,6.67,6.68,6.69,6.7,6.71,6.72,6.73,6.74,6.75,6.7599998,6.77,6.7799997,6.79,6.8,6.8100004,6.82,6.8300004,6.84,6.8500004,6.86,6.8700004,6.88,6.8900003,6.9,6.9100003,6.92,6.9300003,6.94,6.9500003,6.96,6.9700003,6.98,6.9900002,7.0,7.01,7.02,7.03,7.04,7.05,7.06,7.07,7.08,7.09,7.1,7.11,7.12,7.13,7.14,7.15,7.16,7.17,7.18,7.19,7.2,7.21,7.22,7.23,7.24,7.25,7.2599998,7.27,7.28,7.2900004,7.3,7.3100004,7.32,7.3300004,7.34,7.3500004,7.36,7.3700004,7.38,7.3900003,7.4,7.4100003,7.42,7.4300003,7.44,7.4500003,7.46,7.4700003,7.48,7.4900002,7.5,7.51,7.52,7.53,7.54,7.55,7.56,7.57,7.58,7.59,7.6,7.61,7.62,7.63,7.64,7.65,7.66,7.67,7.68,7.69,7.7,7.71,7.72,7.73,7.74,7.75,7.76,7.7700005,7.78,7.7900004,7.8,7.8100004,7.82,7.8300004,7.84,7.8500004,7.86,7.8700004,7.88,7.8900003,7.9,7.9100003,7.92,7.93,7.94,7.95,7.96,7.97,7.98,7.99],\"y\":[-31.0,-30.3633,-29.73317,-29.109596,-28.49255,-27.882004,-27.277939,-26.680325,-26.089153,-25.504383,-24.925995,-24.353977,-23.788292,-23.228912,-22.675823,-22.128994,-21.58842,-21.054047,-20.525871,-20.003864,-19.488007,-18.97826,-18.474613,-17.977028,-17.4855,-17.0,-16.520496,-16.046974,-15.5793915,-15.117748,-14.662003,-14.2121315,-13.768127,-13.329952,-12.897583,-12.470997,-12.050175,-11.635086,-11.225708,-10.8220215,-10.424004,-10.031614,-9.64485,-9.263676,-8.888065,-8.518003,-8.15346,-7.7944107,-7.440832,-7.092701,-6.75,-6.412697,-6.0807705,-5.7541924,-5.4329453,-5.116997,-4.8063374,-4.500931,-4.2007523,-3.9057884,-3.616003,-3.331379,-3.0518913,-2.7775116,-2.5082245,-2.2440014,-1.9848156,-1.73065,-1.481472,-1.2372665,-0.9980011,-0.7636566,-0.5342102,-0.30962944,-0.08990288,0.125,0.33510208,0.540432,0.74100876,0.93685913,1.1279984,1.3144627,1.496273,1.6734467,1.8460121,2.013998,2.1774235,2.3363132,2.4906864,2.6405773,2.7859993,2.9269838,3.063551,3.1957283,3.323535,3.4470024,3.566144,3.6809912,3.7915688,3.8978977,4.0,4.097904,4.1916313,4.281208,4.3666553,4.448001,4.525263,4.5984726,4.6676493,4.732815,4.7940006,4.851224,4.9045124,4.953888,4.9993763,5.0410004,5.078784,5.112753,5.142927,5.1693373,5.1919994,5.210943,5.2261925,5.237768,5.245696,5.25,5.250704,5.2478323,5.2414083,5.2314568,5.2180004,5.201064,5.1806717,5.156849,5.129616,5.099001,5.0650234,5.027712,4.9870877,4.9431763,4.896,4.8455834,4.7919517,4.7351274,4.6751356,4.612,4.545744,4.476392,4.403968,4.328496,4.25,4.1685038,4.084031,3.9966078,3.9062562,3.8130002,3.7168646,3.6178713,3.5160475,3.411416,3.304,3.1938238,3.0809116,2.9652882,2.8469763,2.7259998,2.602384,2.4761524,2.3473277,2.2159357,2.0819998,1.9455438,1.806592,1.6651678,1.521296,1.375,1.2263036,1.075232,0.92180824,0.7660558,0.6079998,0.44766402,0.28507185,0.12024808,-0.046784163,-0.21600008,-0.38737607,-0.56088805,-0.7365122,-0.9142239,-1.0940001,-1.2758161,-1.4596481,-1.645472,-1.8332641,-2.023,-2.214656,-2.408208,-2.603632,-2.800904,-3.0,-3.200896,-3.403568,-3.607992,-3.814144,-4.022,-4.231536,-4.442728,-4.655552,-4.8699837,-5.086,-5.303576,-5.522688,-5.743312,-5.965424,-6.1889997,-6.414016,-6.640448,-6.868272,-7.097464,-7.328,-7.559856,-7.793008,-8.0274315,-8.2631035,-8.5,-8.738096,-8.977367,-9.2177925,-9.459344,-9.702,-9.945736,-10.190527,-10.436352,-10.683184,-10.931,-11.179776,-11.429488,-11.680112,-11.931623,-12.184,-12.437216,-12.691249,-12.946073,-13.201664,-13.458,-13.715055,-13.972808,-14.231232,-14.490303,-14.75,-15.010296,-15.271168,-15.532591,-15.794543,-16.057,-16.319935,-16.583328,-16.84715,-17.111383,-17.376,-17.640976,-17.906288,-18.171913,-18.437824,-18.703999,-18.970415,-19.237047,-19.503872,-19.770864,-20.038,-20.305256,-20.572609,-20.840033,-21.107504,-21.375,-21.642498,-21.90997,-22.177393,-22.444744,-22.712,-22.979137,-23.246128,-23.512953,-23.779585,-24.046,-24.312176,-24.578087,-24.843712,-25.109024,-25.374,-25.638615,-25.902847,-26.166672,-26.430063,-26.693,-26.955456,-27.217407,-27.47883,-27.739702,-28.0,-28.259697,-28.518766,-28.777191,-29.034943,-29.292,-29.548334,-29.803925,-30.05875,-30.312782,-30.565998,-30.818377,-31.06989,-31.320513,-31.570223,-31.819,-32.066814,-32.31365,-32.55947,-32.804268,-33.048,-33.290657,-33.53221,-33.772633,-34.0119,-34.25,-34.486897,-34.72257,-34.95699,-35.190144,-35.421997,-35.652534,-35.881725,-36.10955,-36.335983,-36.560997,-36.784576,-37.006687,-37.227314,-37.446423,-37.664,-37.880013,-38.094448,-38.307274,-38.518467,-38.728,-38.935856,-39.14201,-39.34643,-39.549103,-39.75,-39.949097,-40.14637,-40.34179,-40.535343,-40.727,-40.916733,-41.104523,-41.29035,-41.474182,-41.656,-41.835777,-42.01349,-42.18911,-42.362625,-42.534,-42.703217,-42.870247,-43.035072,-43.197662,-43.358,-43.516052,-43.671806,-43.82523,-43.976303,-44.125,-44.271294,-44.41517,-44.55659,-44.69554,-44.832,-44.965935,-45.09733,-45.226154,-45.352386,-45.475998,-45.596977,-45.71529,-45.83091,-45.943825,-46.054,-46.161415,-46.26605,-46.36787,-46.466866,-46.563,-46.656254,-46.746605,-46.83403,-46.918503,-47.0,-47.0785,-47.15397,-47.22639,-47.295742,-47.362,-47.425133,-47.485126,-47.541954,-47.595585,-47.646,-47.693172,-47.737087,-47.777714,-47.81502,-47.848995,-47.879616,-47.90685,-47.93067,-47.951065,-47.968,-47.981457,-47.991405,-47.99783,-48.000706,-48.0,-47.995697,-47.98777,-47.97619,-47.96094,-47.942,-47.91933,-47.89293,-47.862755,-47.828785,-47.791,-47.749374,-47.703888,-47.65451,-47.601223,-47.544,-47.482815,-47.41765,-47.348473,-47.27527,-47.197998,-47.116657,-47.031208,-46.941628,-46.847904,-46.75,-46.647896,-46.54157,-46.430992,-46.316147,-46.197002,-46.073532,-45.94573,-45.81355,-45.676983,-45.536,-45.390575,-45.24069,-45.086315,-44.927425,-44.763996,-44.596012,-44.423447,-44.246273,-44.06446,-43.87799,-43.686855,-43.491005,-43.29043,-43.085102,-42.875,-42.6601,-42.44037,-42.21579,-41.986347,-41.752,-41.512733,-41.268528,-41.01935,-40.765186,-40.506004,-40.241776,-39.972492,-39.698116,-39.418617,-39.133995,-38.84421,-38.54924,-38.249065,-37.943665,-37.632996,-37.31705,-36.99581,-36.669235,-36.337307,-36.0,-35.6573,-35.309174,-34.955597,-34.596546,-34.232002,-33.86194,-33.486324,-33.10515,-32.718388,-32.326004,-31.927979,-31.524296,-31.114922,-30.699821,-30.27901,-29.852425,-29.420055,-28.981873,-28.537872,-28.087997,-27.632256,-27.170609,-26.703026,-26.229507,-25.75,-25.264503,-24.772972,-24.27539,-23.771751,-23.262009,-22.746132,-22.224129,-21.695961,-21.16159,-20.621002,-20.07418,-19.521088,-18.961723,-18.396027,-17.824005,-17.245628,-16.66085,-16.06968,-15.472069,-14.867996,-14.257454,-13.640404,-13.01683,-12.386696,-11.75,-11.106697,-10.456764,-9.800201,-9.136955,-8.4670105,-7.790344,-7.106941,-6.4167557,-5.7197876,-5.016014,-4.305382,-3.5878906,-2.8635178,-2.1322403,-1.3940125,-0.6488266,0.10333252,0.86252594,1.6287384,2.402008,3.1823502,3.9697876,4.764374,5.5660934,6.375,7.191101,8.014435,8.845009,9.682846,10.528,11.380463,12.240257,13.107445,13.982002,14.863983,15.753418,16.6503,17.55468,18.466576,19.385986,20.312973,21.247566,22.189735,23.139542,24.097,25.062164,26.034996,27.015572,28.003891,29.0,30.003914,31.015617,32.035217,33.062653,34.098038,35.14125,36.192497,37.25164,38.318832,39.393997,40.477226,41.568512,42.667892,43.775368,44.89099,46.014755,47.146767,48.286903,49.43535,50.59198,51.756943,52.930176,54.111763,55.301674,56.5,57.706665,58.921837,60.14537,61.377457,62.61799,63.867065,65.12469,66.390915,67.665634,68.94906,70.24105,71.54176,72.851105,74.169235,75.49602,76.83164,78.17596,79.52916,80.89116,82.26203,83.641754,85.030426,86.427956,87.83455,89.25,90.67454,92.108025,93.550644,95.00224,96.46304,97.93288,99.41189,100.90005,102.397446,103.90399,105.41984,106.944916,108.479294,110.02294,111.57602,113.13835,114.71018,116.29131,117.88197,119.48198,121.09155,122.71057,124.339165,125.977264,127.625,129.28229,130.94922,132.62578,134.31206,136.00803,137.71375,139.42911,141.15431,142.88924,144.63403,146.38864,148.15317,149.92749,151.71185,153.50601,155.31024,157.12436,158.94858,160.78276,162.62704,164.48137,166.34584,168.22037,170.10515,172.0,173.90512,175.82043,177.74606,179.68187,181.62805,183.58446,185.55136,187.52841,189.51604,191.51398,193.5224,195.54132,197.57071,199.61057,201.66101,203.722,205.79355,207.8757,209.96852,212.07196,214.18614,216.31094,218.44656,220.59286,222.75,224.91786,227.09659,229.28622,231.48672,233.69806,235.92035,238.15353,240.39777,242.65285,244.91907,247.19623,249.4846,251.78394,254.09448,256.41608,258.7488,261.09277,263.44797,265.8144,268.19205,270.58093,272.98126,275.39276,277.81573,280.25,282.69574,285.15283,287.62146,290.10144,292.59305,295.09607,297.61075,300.1368,302.67468,305.224,307.78503,310.3577,312.94208,315.53815,318.14606,320.76556,323.39694,326.04007,328.6952,331.36194,334.0407,336.73135,339.434,342.14844,344.875,347.6135,350.36414,353.12668,355.90137,358.68808,361.48694,364.29794,367.12115,369.95648,372.8041,375.6639,378.53607,381.42032,384.31708,387.22604,390.1475,393.0812,396.02737,398.9859,401.95715,404.9405,407.93665,410.94516,413.96637,417.0,420.04633,423.1052,426.17685,429.26105,432.35803,435.4676,438.59012,441.7252,444.87323,448.03394,451.20764,454.39407,457.59348,460.8058,464.03107,467.26917,470.52036,473.78445,477.06177,480.35193,483.6554,486.9717,490.30136,493.64398,497.0,500.36896,503.75143,507.14697,510.55585,513.97784,517.41345,520.8623,524.3246,527.80005,531.2892,534.79144,538.3074,541.8368,545.3797,548.93604,552.50604,556.0895,559.6869,563.29755,566.9222,570.5602,574.21204,577.87756,581.55695,585.25,588.9571,592.6775,596.41223,600.16064,603.9231,607.6993,611.4895,615.2936,619.1119,622.9439,626.7902,630.6505,634.5249,638.4134,642.316,646.23267,650.1638,654.1089,658.0683,662.04193,666.02997,670.0321,674.0488,678.07965,682.125,686.1846,690.2588,694.3473,698.4505,702.568,706.7002,710.8468,715.00806,719.18365,723.3741,727.57904,731.7988,736.0331,740.2823,744.546,748.82477,753.1179,757.4263,761.7492,766.08716,770.43976,774.8074,779.19006,783.5876,788.0,792.4276,796.87,801.32776,805.8003,810.2881,814.7907,819.30884,823.84204,828.3905,832.9539,837.5329,842.12683,846.73645,851.3609,856.001,860.6563,865.3273,870.0133,874.715,879.4319,884.1645,888.9125,893.6763,898.45514,903.25,908.06024,912.8862,917.7279,922.58527,927.4581,932.347,937.2511,942.1714,947.10724,952.0593,957.02655,962.01025,967.00964,972.02496,977.05615,982.10333,987.1664,992.2457,997.34076,1002.45215,1007.5794,1012.7229,1017.88245,1023.0581,1028.25,1033.4583,1038.6824,1043.9232,1049.1798,1054.4531,1059.7424,1065.0483,1070.3705,1075.709,1081.064,1086.4355,1091.8232,1097.2277,1102.6484,1108.086,1113.54,1119.0105,1124.4977,1130.0015,1135.5219,1141.0591,1146.6129,1152.1836,1157.7708,1163.375,1168.9961,1174.6339,1180.2885,1185.96,1191.6481,1197.3535,1203.0756,1208.815,1214.5709,1220.3442,1226.1343,1231.9417,1237.766,1243.6075,1249.4661,1255.342,1261.2349,1267.1449,1273.0723,1279.0168,1284.979,1290.9581,1296.9547,1302.9685],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Function and grad\"},\"width\":800,\"height\":600,\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"f(x), f(x)'\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('9b186108-9a16-45a2-a3a9-ef3670db1470');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"5rx40RnEVkVN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NN in PyTorch 75%\n\nОсновная часть этого домашнего задания будет основана на создании нейросетей для датасета CIFAR10.\nМы создадим и натренируем 3 модели:\n1. Полносвязная нейросеть (Fully Connected). 2 скрытых слоя размером 512 и 128 нейронов. (25%)\n2. Сверточная нейросеть (CNN). 3 сверточных слоя с макс пуллингом и 2 полносвязных слоя. (25%)\n3. Крутая сверточная нейросеть (CNN). Архитектура как вторая, но со всеми приемами которые мы разбирали (Dropout, BatchNorm, Early Stopping, Weight Decay). (25%)\n\n### Функции для тренировки и тестирования\n\nВам нужно будет создать 4 функции, `get_dataloaders` `get_accuracy`, `train_model`, `test_model`.\n\n### Загрузка данных\n\nСначала нам надо создать функцию для загрузки данных `get_dataloaders`. Порядок действий:\n\n1. Создать base_transforms. Они переводят картинку в тензор и нормализуют его.\n2. Создать train_transforms. Они добавляют аугментации для тренировочного датасета.\n3. Загрузить тренировочный и тестовый датасеты (`torchvision.datasets.CIFAR10`).\n4. Разделить тренировочный датасет на тренировочный и валидационный. Для этого можно использовать `torch.utils.data.random_split`, 20 процентов тренировочных картинок должны попасть в валидационный датасет.\n5. Создать даталоадеры для тренировочного, валидационного и тестового датасетов. Для тренировочного датасета `shuffle=True`, для остальных `shuffle=False`.\n6. Вернуть тренировочный, валидационный и тестовый даталоадеры.\n\n\n`num_workers`, `batch_size` и `pin_memory` это параметры даталоадеров, которые могут повлиять на скорость загрузки данных. Почитайте про них в документации PyTorch.","metadata":{"id":"_zslV1Eed9Iu"}},{"cell_type":"code","source":"def get_dataloaders(\n    transform: T.Compose = T.Compose([]),\n    batch_size: int = 256,\n    num_workers: int = 8,\n    pin_memory: bool = True,\n    val_fraction: float = 0.2,\n) -> tuple[DataLoader, DataLoader, DataLoader]:\n    base_transform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\n    train_transform =  T.Compose([T.RandomResizedCrop(32, scale=(0.9, 1.1), antialias=True),\n                                  T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                                  T.RandomRotation(15)])\n    \n    transform = T.Compose([train_transform, base_transform])\n   \n\n    # load the data\n    trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n    \n    testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=base_transform)\n\n    val_size = int(0.2 * len(trainset))\n    train_size = len(trainset) - val_size\n    trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\n    return trainloader, valloader, testloader\n\n# проверка\nbatch_size = 4\ntrainloader, valloader, testloader = get_dataloaders(batch_size=batch_size)\nimages, labels = next(iter(trainloader))\nassert images.size() == (batch_size, 3, 32, 32)\nassert labels.size() == (batch_size,)\n\n# / 2 + 0.5 - это чтобы перевести значения из диапазона [-1, 1] в [0, 1]\nplt.imshow(torchvision.utils.make_grid(images).permute(1, 2, 0).numpy() / 2 + 0.5)\n","metadata":{"id":"EUg44v3gd9Iu","outputId":"a94f3234-4eaf-4866-b993-52e08cfd93a1","execution":{"iopub.status.busy":"2024-05-07T17:23:35.620479Z","iopub.execute_input":"2024-05-07T17:23:35.620855Z","iopub.status.idle":"2024-05-07T17:23:38.343799Z","shell.execute_reply.started":"2024-05-07T17:23:35.620826Z","shell.execute_reply":"2024-05-07T17:23:38.342666Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning:\n\nThis DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7caaafcd5f30>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"images, labels = next(iter(testloader))\nplt.imshow(torchvision.utils.make_grid(images).permute(1, 2, 0).numpy() / 2 + 0.5)","metadata":{"id":"us-iG-DACIBX","outputId":"65e6510f-78ea-46ff-8b0f-4ba788eb9b76","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### `get_accuracy`\n\nФункция должна считать точность (количество правильных ответов / общее количество ответов). Пригодятся `tensor.argmax`, сравнение тензоров, `tensor.item`, `tensor.sum`.","metadata":{"id":"n_TBlxlOd9Iu"}},{"cell_type":"code","source":"def get_accuracy(outputs: torch.Tensor, labels: torch.Tensor) -> float:\n    \"\"\"\n    Outputs - предсказания модели (batch_size, n_classes)\n    Labels - истинные значения (batch_size, 1)\n    \"\"\"\n    preds = outputs.argmax(dim=1)\n    correct = (preds == labels).sum().item()\n    total = len(labels)\n    acc = correct / total\n    return acc\n\n# проверка\noutputs = torch.tensor([[0.1, 0.2, 0.7], [0.9, 0.05, 0.05], [0.2, 0.2, 0.6]])\nlabels = torch.tensor([2, 0, 2])\n\nassert get_accuracy(outputs, labels) == 1","metadata":{"id":"FBo-wCe6d9Iv","execution":{"iopub.status.busy":"2024-05-07T12:03:30.483469Z","iopub.execute_input":"2024-05-07T12:03:30.483820Z","iopub.status.idle":"2024-05-07T12:03:30.492386Z","shell.execute_reply.started":"2024-05-07T12:03:30.483794Z","shell.execute_reply":"2024-05-07T12:03:30.491493Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### `train_model`\n\nОсновная функция для тренировки модели. На вход подается модель, даталоадеры для тртренировочного и валидационного датасетов.\nНа ваше усмотрения можете добавить все гиперпараметры, которые считаете нужными (learning rate, num epochs, weight decay etc).\n\nПорядок действий:\n\n1. Перевести модель в режим тренировки\n2. Создать критерион для подсчета ошибки\n3. Создать оптимизатор (Adam)\n4. Переместить модель на gpu (если доступно)\n5. Создать словарь для метрик (train_loss, train_acc, val_loss, val_acc)\n6. Создать цикл для количества эпох\n7. Внутри цикла создать цикл для тренировочного и валидационного даталоадеров\n8. Внутри цикла для тренировочного даталоадера:\n    1. Обнулить градиенты\n    2. Переместить данные на gpu\n    3. Посчитать предсказания\n    4. Посчитать ошибку\n    5. Посчитать градиенты\n    6. Сделать шаг оптимизатора\n    7. Сохранить метрики\n 9. Внутри цикла для валидационного даталоадера:\n    1. Перевести модель в режим валидации\n    2. Посчитать предсказания\n    3. Посчитать ошибку\n    4. Сохранить метрики","metadata":{"id":"s3Drrhwld9Iv"}},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T17:23:49.793302Z","iopub.execute_input":"2024-05-07T17:23:49.793702Z","iopub.status.idle":"2024-05-07T17:23:49.800598Z","shell.execute_reply.started":"2024-05-07T17:23:49.793649Z","shell.execute_reply":"2024-05-07T17:23:49.799618Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def get_string_output(epoch: int, metrics:dict[str, list]) -> str:\n    \"\"\"Print the epoch results\"\"\"\n    train_loss = metrics['train_loss'][epoch]\n    val_loss = metrics['val_loss'][epoch]\n    train_acc = metrics['train_acc'][epoch]\n    val_acc = metrics['val_acc'][epoch]\n    loss_string = f\"[Epoch {epoch + 1}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n    acc_string = f\"Train Accuracy: {train_acc * 100:.2f}%, Val Accuracy: {val_acc * 100:.2f}%\"\n    return loss_string + acc_string\n\ndef train_model(\n    model: nn.Module,\n    trainloader: DataLoader,\n    valloader: DataLoader,\n    num_epochs: int = 10,\n    lr: float = 1e-3,\n) -> dict[str, list[float]]:\n    metrics = dict(train_loss=[], val_loss=[], train_acc=[], val_acc=[])\n    # move model to GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    # init loss, optimiser\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    for epoch in range(num_epochs):\n        # save metrics from each batch to calculate mean per epoch\n        epoch_metrics = defaultdict(list)\n        model.train()\n        # Train loop\n        for data in trainloader:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n            outputs = model.forward(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            acc = get_accuracy(outputs, labels)\n            epoch_metrics['train_acc'].append(acc)\n            epoch_metrics['train_loss'].append(loss.item())\n\n        # Validation loop\n        model.eval()\n        with torch.no_grad():\n            for data in valloader:\n                inputs, labels = data[0].to(device), data[1].to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                acc = get_accuracy(outputs, labels)\n                epoch_metrics['val_acc'].append(acc)\n                epoch_metrics['val_loss'].append(loss.item())\n        # Housekeeping\n        for k,v in epoch_metrics.items():\n            metrics[k].append(np.mean(v))\n        print(get_string_output(epoch, metrics))\n    return metrics","metadata":{"id":"l8Pi4SNNd9Iv","execution":{"iopub.status.busy":"2024-05-07T12:00:45.439331Z","iopub.execute_input":"2024-05-07T12:00:45.440153Z","iopub.status.idle":"2024-05-07T12:00:45.454685Z","shell.execute_reply.started":"2024-05-07T12:00:45.440122Z","shell.execute_reply":"2024-05-07T12:00:45.453744Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### `test_model`\n\nФункция для тестирования модели. На вход подается модель и даталоадер для тестового датасета.\n\nПорядок действий похож на `train_model`, но с некоторыми отличиями:\n\n* Не нужно обновлять веса модели\n* Не нужно считать градиенты\n* Соответсвенно, не нужен оптимизатор\n* Нет цикла для эпохов\n* Только один цикл для даталоадера\n  \nФункция должна вернуть точность и лосс на тестовом датасете.","metadata":{"id":"lzD8etTjd9Iv"}},{"cell_type":"code","source":"def test_model(model: nn.Module, testloader: DataLoader) -> tuple[float, float]:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n    acc = []\n    loss_report = []\n    criterion = nn.CrossEntropyLoss()\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data[0].to(device), data[1].to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            acc.append(get_accuracy(outputs, labels))\n            loss_report.append(loss.item())\n    return np.mean(loss_report), np.mean(acc)","metadata":{"id":"0rU_fmzPd9Iv","execution":{"iopub.status.busy":"2024-05-07T12:00:51.031124Z","iopub.execute_input":"2024-05-07T12:00:51.031441Z","iopub.status.idle":"2024-05-07T12:00:51.039236Z","shell.execute_reply.started":"2024-05-07T12:00:51.031416Z","shell.execute_reply":"2024-05-07T12:00:51.038141Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Проверка функций\n\nПосле этого можно проверить функции `train_model` и `test_model`.\nДля этого у вас есть модель `BadModel` (что она делает?).\n\nПопробуйте \"натренировать\" эту модель на одной эпохе и проверить работоспособность функций.","metadata":{"id":"ZkuefNx0d9Iv"}},{"cell_type":"code","source":"%%time\nclass BadModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Linear(1, 10)\n\n    def forward(self, x):\n        xmean = x.mean(dim=(1, 2, 3)).unsqueeze(1)\n        x = self.layer(xmean)\n        return x\n\n\ntrainloader, valloader, testloader = get_dataloaders()\nbad_model = BadModel()\ntrain_metrics = train_model(bad_model, trainloader, valloader, num_epochs=5)\ntest_loss, test_acc = test_model(bad_model, testloader)\nprint(f\"Test Loss: {test_loss:.4f}, Test acc: {test_acc * 100:.2f}%\")","metadata":{"id":"_03dDEcdd9Iv","outputId":"81d33a3f-bd7b-45a7-bd13-2f93d805bb72","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Создание моделей\n\nЕсли все прошло успешно, можно переходить к созданию моделей.\n\n#### Полносвязная нейросеть\n\nПараметры - 2 скрытых слоя размером 512 и 128 нейронов. Активация - ReLU.\n\nИспользуйте `nn.Linear`, `torch.relu`, `tensor.flatten` или `tensor.view`.\n\nНатренируйте модель на 30 эпохах. Посмотрите на точность на тренировочном, валидационном и тестовом датасетах.\n\nПостройте график лосса и точности на тренировочном и валидационном датасетах.\n\n**Вопросы:**\n\n1. Какая точность на тестовом датасете?\n2. Как быстро сходится модель?\n3. Происходит ли переобучение?\n4. Если да, то в какой момент?","metadata":{"id":"KTCpFViMd9Iv"}},{"cell_type":"code","source":"%%time\nclass SimpleNN(nn.Module):\n    def __init__(self, image_size : int = 32, channels: int = 3): #CIFAR10 - image size 32x32x3\n        super().__init__()\n\n        self.fc1 = nn.Linear(image_size * image_size * channels, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x:torch.Tensor) -> torch.Tensor:\n        x = x.flatten(1, -1)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ntrainloader, valloader, testloader = get_dataloaders()\nmodel = SimpleNN()\ntrain_metrics_simple = train_model(model, trainloader, valloader, num_epochs=30)\ntest_loss_simple, test_acc_simple = test_model(model, testloader)\nprint(f\"Test Loss: {test_loss_simple:.4f}, Test acc: {test_acc_simple * 100:.2f}%\")","metadata":{"id":"aGisgeYEsRHR","outputId":"18f06f06-0baa-4ce0-9020-bf13676d58b1","execution":{"iopub.status.busy":"2024-05-05T05:49:56.239389Z","iopub.execute_input":"2024-05-05T05:49:56.239769Z","iopub.status.idle":"2024-05-05T06:02:51.521012Z","shell.execute_reply.started":"2024-05-05T05:49:56.239738Z","shell.execute_reply":"2024-05-05T06:02:51.519851Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n[Epoch 1] Train Loss: 1.7643, Val Loss: 1.6423, Train Accuracy: 37.12%, Val Accuracy: 42.27%\n[Epoch 2] Train Loss: 1.5781, Val Loss: 1.5706, Train Accuracy: 44.09%, Val Accuracy: 44.40%\n[Epoch 3] Train Loss: 1.4952, Val Loss: 1.5178, Train Accuracy: 46.73%, Val Accuracy: 46.13%\n[Epoch 4] Train Loss: 1.4403, Val Loss: 1.4994, Train Accuracy: 49.04%, Val Accuracy: 47.18%\n[Epoch 5] Train Loss: 1.3968, Val Loss: 1.4824, Train Accuracy: 50.44%, Val Accuracy: 47.63%\n[Epoch 6] Train Loss: 1.3605, Val Loss: 1.4502, Train Accuracy: 51.54%, Val Accuracy: 49.22%\n[Epoch 7] Train Loss: 1.3311, Val Loss: 1.4437, Train Accuracy: 52.63%, Val Accuracy: 49.77%\n[Epoch 8] Train Loss: 1.3027, Val Loss: 1.4479, Train Accuracy: 53.97%, Val Accuracy: 49.45%\n[Epoch 9] Train Loss: 1.2768, Val Loss: 1.4127, Train Accuracy: 54.60%, Val Accuracy: 49.90%\n[Epoch 10] Train Loss: 1.2572, Val Loss: 1.4530, Train Accuracy: 55.19%, Val Accuracy: 49.62%\n[Epoch 11] Train Loss: 1.2356, Val Loss: 1.4188, Train Accuracy: 56.02%, Val Accuracy: 50.48%\n[Epoch 12] Train Loss: 1.2234, Val Loss: 1.4162, Train Accuracy: 56.44%, Val Accuracy: 51.34%\n[Epoch 13] Train Loss: 1.1977, Val Loss: 1.4231, Train Accuracy: 57.10%, Val Accuracy: 50.76%\n[Epoch 14] Train Loss: 1.1817, Val Loss: 1.4168, Train Accuracy: 57.91%, Val Accuracy: 51.46%\n[Epoch 15] Train Loss: 1.1661, Val Loss: 1.4065, Train Accuracy: 58.86%, Val Accuracy: 51.61%\n[Epoch 16] Train Loss: 1.1504, Val Loss: 1.4137, Train Accuracy: 59.15%, Val Accuracy: 52.06%\n[Epoch 17] Train Loss: 1.1387, Val Loss: 1.4096, Train Accuracy: 59.39%, Val Accuracy: 51.46%\n[Epoch 18] Train Loss: 1.1278, Val Loss: 1.3856, Train Accuracy: 59.93%, Val Accuracy: 52.36%\n[Epoch 19] Train Loss: 1.1138, Val Loss: 1.4172, Train Accuracy: 60.35%, Val Accuracy: 51.66%\n[Epoch 20] Train Loss: 1.0942, Val Loss: 1.4058, Train Accuracy: 60.89%, Val Accuracy: 52.36%\n[Epoch 21] Train Loss: 1.0836, Val Loss: 1.4449, Train Accuracy: 61.42%, Val Accuracy: 51.46%\n[Epoch 22] Train Loss: 1.0686, Val Loss: 1.4180, Train Accuracy: 61.98%, Val Accuracy: 52.05%\n[Epoch 23] Train Loss: 1.0523, Val Loss: 1.4394, Train Accuracy: 62.37%, Val Accuracy: 51.35%\n[Epoch 24] Train Loss: 1.0507, Val Loss: 1.4245, Train Accuracy: 62.41%, Val Accuracy: 52.35%\n[Epoch 25] Train Loss: 1.0349, Val Loss: 1.4247, Train Accuracy: 62.95%, Val Accuracy: 52.48%\n[Epoch 26] Train Loss: 1.0226, Val Loss: 1.4371, Train Accuracy: 63.49%, Val Accuracy: 53.39%\n[Epoch 27] Train Loss: 1.0108, Val Loss: 1.4483, Train Accuracy: 64.04%, Val Accuracy: 53.03%\n[Epoch 28] Train Loss: 1.0053, Val Loss: 1.4331, Train Accuracy: 64.31%, Val Accuracy: 52.74%\n[Epoch 29] Train Loss: 0.9966, Val Loss: 1.4446, Train Accuracy: 64.63%, Val Accuracy: 53.22%\n[Epoch 30] Train Loss: 0.9829, Val Loss: 1.4634, Train Accuracy: 64.85%, Val Accuracy: 52.17%\nTest Loss: 1.4687, Test acc: 52.56%\nCPU times: user 30.9 s, sys: 15.1 s, total: 46 s\nWall time: 12min 55s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"w1jiBY5sQVWS","outputId":"e17f1f82-b782-47cb-88ac-f50e3a6fd07c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_accuracy_loss(p_train_metrics : defaultdict(list)) :\n  fig = go.Figure()\n  x = x=torch.arange(1, len(p_train_metrics['train_acc']))\n  fig.add_trace(go.Scatter(x=x, y=p_train_metrics['train_acc'], mode='lines', name='Train accuracy', opacity=0.7))\n  fig.add_trace(go.Scatter(x=x, y=p_train_metrics['val_acc'], mode='lines', name='Validation accuracy', opacity=0.7))\n  fig.add_trace(go.Scatter(x=x, y=p_train_metrics['train_loss'], mode='lines', name='Train loss', opacity=0.7))\n  fig.add_trace(go.Scatter(x=x, y=p_train_metrics['val_loss'], mode='lines', name='Validation loss', opacity=0.7))\n  fig.update_layout(title='Accuracy & loss', width=800, height=600, xaxis_title='Epoch', yaxis_title='Accuracy & loss')\n  fig.show()\n\nplot_accuracy_loss(train_metrics_simple)","metadata":{"id":"GuvSz0LTVy1T","outputId":"233a8311-6909-4a2d-e24f-8c12cc1a9383","execution":{"iopub.status.busy":"2024-05-05T06:02:52.870480Z","iopub.execute_input":"2024-05-05T06:02:52.870816Z","iopub.status.idle":"2024-05-05T06:02:53.038996Z","shell.execute_reply.started":"2024-05-05T06:02:52.870768Z","shell.execute_reply":"2024-05-05T06:02:53.038071Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"de7bfc1e-3e25-4a04-9220-6c77c7e8bf88\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"de7bfc1e-3e25-4a04-9220-6c77c7e8bf88\")) {                    Plotly.newPlot(                        \"de7bfc1e-3e25-4a04-9220-6c77c7e8bf88\",                        [{\"mode\":\"lines\",\"name\":\"Train accuracy\",\"opacity\":0.7,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.37116839171974525,0.44090863853503187,0.46730692675159236,0.4903712181528662,0.5044287420382165,0.5154010748407644,0.5263485270700637,0.5397093949044586,0.5459792993630573,0.55187599522293,0.5601861066878981,0.564390923566879,0.5709593949044586,0.5791202229299363,0.5886494824840764,0.5914609872611465,0.593874402866242,0.5992983678343949,0.6034783041401274,0.6089271496815286,0.6142267117834395,0.6198497213375797,0.6236813296178344,0.6240794187898089,0.6295033837579618,0.6349273487261147,0.6404010748407644,0.6430881767515924,0.6463475318471338,0.6484872611464968],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation accuracy\",\"opacity\":0.7,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.42265625,0.44404296875,0.461328125,0.47177734375,0.47626953125,0.4921875,0.49765625,0.49453125,0.4990234375,0.49619140625,0.50478515625,0.51337890625,0.5076171875,0.51455078125,0.51611328125,0.52060546875,0.51455078125,0.5236328125,0.5166015625,0.5236328125,0.51455078125,0.5205078125,0.5134765625,0.52353515625,0.5248046875,0.53388671875,0.5302734375,0.52744140625,0.5322265625,0.5216796875],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Train loss\",\"opacity\":0.7,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[1.7643380377702653,1.578070541855636,1.4951699250822614,1.440342152953907,1.3968019705669137,1.360515266467052,1.3311173452693186,1.3027258344516632,1.2768421727380934,1.2572279948337821,1.2356299875648158,1.2234106671278644,1.1977015065539414,1.181709159711364,1.1660534860981497,1.1504375125951827,1.1387384788245911,1.1278444756368162,1.1138132689105478,1.0942280455759377,1.0836085726501077,1.0685608227541492,1.0523321772836576,1.0507251570938498,1.034866107876893,1.0226217709529173,1.0107710327312445,1.0052991809359022,0.9965991533485947,0.982870897669701],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation loss\",\"opacity\":0.7,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[1.6423051297664641,1.5705655306577682,1.5178355991840362,1.4994416207075119,1.4824075132608414,1.4501823693513871,1.4436641931533813,1.4478928595781326,1.4126855939626695,1.453037717938423,1.418833428621292,1.41615172624588,1.4230696320533753,1.4168033093214034,1.406545752286911,1.4137416392564774,1.4096166163682937,1.3855793207883835,1.4171772480010987,1.4057822406291962,1.444940096139908,1.418033054471016,1.4394492268562318,1.4244720786809921,1.4246870547533035,1.4371213257312774,1.4483467847108842,1.4331295490264893,1.4446421444416047,1.4634208709001542],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Accuracy & loss\"},\"width\":800,\"height\":600,\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy & loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('de7bfc1e-3e25-4a04-9220-6c77c7e8bf88');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"Yw7GHbF8WliD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Сверточная нейросеть\n\nПараметры - 3 сверточных слоя с макс пуллингом и 2 полносвязных слоя. Активацонная функция тоже ReLU.\n\nИспользуйте `nn.Conv2d`, `nn.MaxPool2d`, остальные функции как в предыдущей модели.\n\nДля сверточных слоев используйте `kernel_size=3`, `padding=1`, `stride=1`. Для макс пуллинга `kernel_size=2`, `stride=2`.\n\nНатренируйте модель на 30 эпохах. Посмотрите на точность на тренировочном, валидационном и тестовом датасетах.\n\nПостройте график лосса и точности на тренировочном и валидационном датасетах.\n\n**Вопросы:**\n\n1. Какая точность на тестовом датасете?\n2. Как быстро сходится модель?\n3. Происходит ли переобучение?\n4. Если да, то в какой момент?\n5. Какая модель лучше, полносвязная или сверточная?","metadata":{"id":"pnKh9kyHd9Iv"}},{"cell_type":"code","source":"%%time\nclass SimpleCNN(nn.Module):\n    def __init__(self, image_size : int = 32, channels: int = 3):\n        super().__init__()\n        self.image_size = image_size\n        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)\n\n    def forward(self, x:torch.Tensor) -> torch.Tensor:\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.pool(torch.relu(self.conv3(x)))\n        x = x.flatten(1, -1)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ntrainloader, valloader, testloader = get_dataloaders(num_workers = 4)\nmodel = SimpleCNN()\ntrain_metrics_simple_cnn = train_model(model, trainloader, valloader, num_epochs=30)\ntest_loss_simple_cnn, test_acc_simple_cnn = test_model(model, testloader)\nprint(f\"Test Loss: {test_loss_simple_cnn:.4f}, Test acc: {test_acc_simple_cnn * 100:.2f}%\")","metadata":{"id":"6QL655Tpd9Iv","outputId":"be694f2d-db88-47b8-8e44-07b71f3a81fe","execution":{"iopub.status.busy":"2024-05-05T06:02:53.041085Z","iopub.execute_input":"2024-05-05T06:02:53.041375Z","iopub.status.idle":"2024-05-05T06:16:37.087143Z","shell.execute_reply.started":"2024-05-05T06:02:53.041350Z","shell.execute_reply":"2024-05-05T06:16:37.085983Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning:\n\nThis DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Train Loss: 1.7177, Val Loss: 1.4548, Train Accuracy: 36.49%, Val Accuracy: 47.06%\n[Epoch 2] Train Loss: 1.3343, Val Loss: 1.2548, Train Accuracy: 52.03%, Val Accuracy: 54.56%\n[Epoch 3] Train Loss: 1.1625, Val Loss: 1.0982, Train Accuracy: 58.51%, Val Accuracy: 60.32%\n[Epoch 4] Train Loss: 1.0351, Val Loss: 1.0226, Train Accuracy: 63.29%, Val Accuracy: 63.90%\n[Epoch 5] Train Loss: 0.9402, Val Loss: 0.9798, Train Accuracy: 66.77%, Val Accuracy: 65.78%\n[Epoch 6] Train Loss: 0.8657, Val Loss: 0.8854, Train Accuracy: 69.80%, Val Accuracy: 68.59%\n[Epoch 7] Train Loss: 0.8044, Val Loss: 0.9035, Train Accuracy: 71.80%, Val Accuracy: 68.62%\n[Epoch 8] Train Loss: 0.7582, Val Loss: 0.8932, Train Accuracy: 73.21%, Val Accuracy: 69.79%\n[Epoch 9] Train Loss: 0.7046, Val Loss: 0.8512, Train Accuracy: 75.28%, Val Accuracy: 70.63%\n[Epoch 10] Train Loss: 0.6586, Val Loss: 0.8466, Train Accuracy: 76.79%, Val Accuracy: 70.99%\n[Epoch 11] Train Loss: 0.6249, Val Loss: 0.8694, Train Accuracy: 78.00%, Val Accuracy: 71.11%\n[Epoch 12] Train Loss: 0.5871, Val Loss: 0.8583, Train Accuracy: 79.22%, Val Accuracy: 71.41%\n[Epoch 13] Train Loss: 0.5532, Val Loss: 0.8053, Train Accuracy: 80.33%, Val Accuracy: 72.87%\n[Epoch 14] Train Loss: 0.5117, Val Loss: 0.8334, Train Accuracy: 82.02%, Val Accuracy: 72.34%\n[Epoch 15] Train Loss: 0.4847, Val Loss: 0.8501, Train Accuracy: 82.90%, Val Accuracy: 72.33%\n[Epoch 16] Train Loss: 0.4604, Val Loss: 0.8405, Train Accuracy: 83.66%, Val Accuracy: 73.55%\n[Epoch 17] Train Loss: 0.4273, Val Loss: 0.8385, Train Accuracy: 84.85%, Val Accuracy: 73.53%\n[Epoch 18] Train Loss: 0.4054, Val Loss: 0.9193, Train Accuracy: 85.61%, Val Accuracy: 72.09%\n[Epoch 19] Train Loss: 0.3952, Val Loss: 0.8860, Train Accuracy: 85.96%, Val Accuracy: 72.85%\n[Epoch 20] Train Loss: 0.3650, Val Loss: 0.9087, Train Accuracy: 87.10%, Val Accuracy: 72.73%\n[Epoch 21] Train Loss: 0.3464, Val Loss: 0.9299, Train Accuracy: 87.88%, Val Accuracy: 72.48%\n[Epoch 22] Train Loss: 0.3279, Val Loss: 0.8977, Train Accuracy: 88.38%, Val Accuracy: 73.16%\n[Epoch 23] Train Loss: 0.3062, Val Loss: 0.9490, Train Accuracy: 89.20%, Val Accuracy: 71.88%\n[Epoch 24] Train Loss: 0.3057, Val Loss: 0.9107, Train Accuracy: 89.31%, Val Accuracy: 73.14%\n[Epoch 25] Train Loss: 0.2862, Val Loss: 0.9142, Train Accuracy: 89.88%, Val Accuracy: 74.31%\n[Epoch 26] Train Loss: 0.2676, Val Loss: 0.9580, Train Accuracy: 90.64%, Val Accuracy: 73.25%\n[Epoch 27] Train Loss: 0.2559, Val Loss: 1.0370, Train Accuracy: 90.89%, Val Accuracy: 72.82%\n[Epoch 28] Train Loss: 0.2479, Val Loss: 1.0039, Train Accuracy: 91.28%, Val Accuracy: 73.05%\n[Epoch 29] Train Loss: 0.2410, Val Loss: 1.0375, Train Accuracy: 91.52%, Val Accuracy: 72.83%\n[Epoch 30] Train Loss: 0.2296, Val Loss: 1.0262, Train Accuracy: 91.91%, Val Accuracy: 73.24%\nTest Loss: 0.9971, Test acc: 76.76%\nCPU times: user 1min 11s, sys: 17.1 s, total: 1min 28s\nWall time: 13min 44s\n","output_type":"stream"}]},{"cell_type":"code","source":"plot_accuracy_loss(train_metrics_simple_cnn)","metadata":{"id":"5-8azfkaCIBb","outputId":"c7c0040e-4be4-4778-90a5-7e167a07841a","execution":{"iopub.status.busy":"2024-05-05T06:16:37.089174Z","iopub.execute_input":"2024-05-05T06:16:37.089658Z","iopub.status.idle":"2024-05-05T06:16:37.107099Z","shell.execute_reply.started":"2024-05-05T06:16:37.089617Z","shell.execute_reply":"2024-05-05T06:16:37.106110Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"42c1ad3d-f0bb-4b23-9bc8-612a0e99407b\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"42c1ad3d-f0bb-4b23-9bc8-612a0e99407b\")) {                    Plotly.newPlot(                        \"42c1ad3d-f0bb-4b23-9bc8-612a0e99407b\",                        [{\"mode\":\"lines\",\"name\":\"Train accuracy\",\"opacity\":0.7,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.36494824840764334,0.5203274283439491,0.5850915605095541,0.6328622611464968,0.6676950636942676,0.697999601910828,0.7179787022292994,0.7320859872611465,0.75281150477707,0.7678642515923567,0.7799810907643312,0.7921974522292994,0.803343949044586,0.8201632165605095,0.8290207006369427,0.8366341560509554,0.8485021894904459,0.8561156449044586,0.8596238057324841,0.8709942277070064,0.8788316082802548,0.8838077229299363,0.891968550955414,0.8931130573248408,0.8988355891719745,0.9064490445859873,0.908937101910828,0.9127687101910829,0.91515724522293,0.9190883757961783],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation accuracy\",\"opacity\":0.7,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.47060546875,0.54560546875,0.60322265625,0.63896484375,0.6578125,0.6859375,0.68623046875,0.6978515625,0.70634765625,0.70986328125,0.7111328125,0.7140625,0.7287109375,0.7234375,0.72333984375,0.735546875,0.73525390625,0.7208984375,0.728515625,0.72734375,0.7248046875,0.731640625,0.71875,0.7314453125,0.74306640625,0.73251953125,0.72822265625,0.73046875,0.7283203125,0.732421875],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Train loss\",\"opacity\":0.7,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[1.7176891701995947,1.3342933487740292,1.1624799108808967,1.0351107310337626,0.9401795393342425,0.8656752215828866,0.8044122058874482,0.758228110659654,0.7046200336924024,0.6586001564742653,0.6249466990209689,0.5870854514799301,0.5531574024516306,0.5117191212951757,0.4846545457839966,0.4604188050054441,0.4272751483567961,0.4054234435983524,0.3951527876838757,0.36497044563293457,0.3464413694325526,0.32793448115610013,0.30620840324717724,0.30574631776399674,0.28619797432878213,0.26756643489667564,0.255870971899883,0.2478927223925378,0.24103950879945876,0.22959175677436172],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Validation loss\",\"opacity\":0.7,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[1.454835295677185,1.2548327416181564,1.098157599568367,1.0226013243198395,0.9798396676778793,0.8853717237710953,0.9035323709249496,0.893243807554245,0.8511597707867622,0.8465837582945823,0.869351701438427,0.8582613304257393,0.8052849724888802,0.8334069132804871,0.8501207768917084,0.8405364483594895,0.8385380074381829,0.9193286195397377,0.8859762400388718,0.9087140083312988,0.9299359813332557,0.897677306830883,0.9490013048052788,0.9106603428721428,0.9142316706478596,0.9580025516450406,1.0369843706488608,1.0039352461695672,1.037488430738449,1.0261985689401627],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Accuracy & loss\"},\"width\":800,\"height\":600,\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy & loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('42c1ad3d-f0bb-4b23-9bc8-612a0e99407b');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Крутая сверточная нейросеть\n\nЗадание аналогично предыдущему, но добавьте все приемы, которые мы разбирали (Dropout, BatchNorm, Early Stopping, Weight Decay, Data Augmentation).\nВозможно Вам придется переписать функции `get_dataloaders`, `train_model` и `test_model`.\n\nЦель - точность в 80% на тестовом датасете. Ограничение в 30 эпох.","metadata":{"id":"I4symsXPd9Iv"}},{"cell_type":"markdown","source":"","metadata":{"id":"DHfJFmLCd9Iw"}},{"cell_type":"code","source":"def get_dataloaders_cnn(\n    transform: T.Compose = T.Compose([]),\n    batch_size: int = 256,\n    num_workers: int = 8,\n    pin_memory: bool = True,\n    val_fraction: float = 0.2,\n) -> tuple[DataLoader, DataLoader, DataLoader]:\n    base_transform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\n    train_transform_1 =  T.Compose([T.RandomResizedCrop(32, scale=(0.9, 1.1), antialias=True),\n                                  T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                                  T.RandomRotation(15)])\n    \n    train_transform_2 =  T.Compose([T.RandomHorizontalFlip(0.7), T.RandomResizedCrop(32, scale=(0.9, 1.1), antialias=True),\n                                  T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                                  T.RandomRotation(30)])\n    \n    # 2 трансформации под разные augumentation\n    transform_1 = T.Compose([train_transform_1, base_transform])\n    transform_2 = T.Compose([train_transform_2, base_transform])\n\n    # load the data\n    trainset_1 = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_1)\n    trainset_2 = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_2)\n    testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=base_transform)\n    \n    # объединяем 2 трэйнсета\n    trainset = ConcatDataset([trainset_1, trainset_2])\n    val_size = int(val_fraction * len(trainset))\n    train_size = len(trainset) - val_size\n    trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\n    return trainloader, valloader, testloader","metadata":{"execution":{"iopub.status.busy":"2024-05-07T12:01:15.484722Z","iopub.execute_input":"2024-05-07T12:01:15.485367Z","iopub.status.idle":"2024-05-07T12:01:15.497965Z","shell.execute_reply.started":"2024-05-07T12:01:15.485333Z","shell.execute_reply":"2024-05-07T12:01:15.497007Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping():\n    def __init__(self, patience=5, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n    def __call__(self, val_loss):\n        if self.best_loss == None:\n            self.best_loss = val_loss\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            # reset counter if validation loss improves\n            self.counter = 0\n        elif self.best_loss - val_loss < self.min_delta:\n            self.counter += 1\n            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter >= self.patience:\n                print('INFO: Early stopping')\n                self.early_stop = True","metadata":{"execution":{"iopub.status.busy":"2024-05-07T12:01:21.037094Z","iopub.execute_input":"2024-05-07T12:01:21.037653Z","iopub.status.idle":"2024-05-07T12:01:21.044459Z","shell.execute_reply.started":"2024-05-07T12:01:21.037620Z","shell.execute_reply":"2024-05-07T12:01:21.043690Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%time\ndef train_model_cnn(\n    model: nn.Module,\n    trainloader: DataLoader,\n    valloader: DataLoader,\n    num_epochs: int = 10,\n    learning_rate: float = 1e-3,\n    weight_decay = 5e-4,\n    patience = 5, \n) -> dict[str, list[float]]:\n    metrics = dict(train_loss=[], val_loss=[], train_acc=[], val_acc=[])\n    # move model to GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    # init loss, optimiser\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n    # Параметры LR, выбираем экспоненциальное уменьшение\n    learning_rate_decay_factor = 0.9\n    learning_rate_decay_step = 3\n    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=learning_rate_decay_factor, last_epoch=-1)\n    \n    early_stopping = EarlyStopping(patience = patience)\n    \n    for epoch in range(num_epochs):\n        # save metrics from each batch to calculate mean per epoch\n        epoch_metrics = defaultdict(list)\n        model.train()\n        # Train loop\n        for data in trainloader:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n            outputs = model.forward(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            acc = get_accuracy(outputs, labels)\n            epoch_metrics['train_acc'].append(acc)\n            epoch_metrics['train_loss'].append(loss.item())\n        # Уменьшение lr согласно выбранной стратегии\n        if epoch % learning_rate_decay_step == 0:\n            lr_scheduler.step()\n        print(f'Epoch {epoch} learning rate {lr_scheduler.get_last_lr()}')\n\n        # Validation loop\n        model.eval()\n        with torch.no_grad():\n            for data in valloader:\n                inputs, labels = data[0].to(device), data[1].to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                acc = get_accuracy(outputs, labels)\n                epoch_metrics['val_acc'].append(acc)\n                epoch_metrics['val_loss'].append(loss.item())\n                \n        # Используем early stopping\n        early_stopping(np.mean(epoch_metrics['val_loss']))\n        \n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        # Housekeeping\n        for k,v in epoch_metrics.items():\n            metrics[k].append(np.mean(v))\n        print(get_string_output(epoch, metrics))\n    return metrics","metadata":{"id":"HaIW6DBBa3Pk","execution":{"iopub.status.busy":"2024-05-07T12:01:27.678653Z","iopub.execute_input":"2024-05-07T12:01:27.679571Z","iopub.status.idle":"2024-05-07T12:01:27.693666Z","shell.execute_reply.started":"2024-05-07T12:01:27.679523Z","shell.execute_reply":"2024-05-07T12:01:27.692693Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 9 µs, sys: 2 µs, total: 11 µs\nWall time: 21.7 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nclass ModifiedCNN(nn.Module):\n    def __init__(self, image_size : int = 32, channels: int = 3, p_dropout: float = 0.25):\n        super().__init__()\n        self.image_size = image_size\n        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.batch32 = nn.BatchNorm2d(32)\n        self.batch64 = nn.BatchNorm2d(64)\n        self.batch128 = nn.BatchNorm2d(128)\n        self.batch_fc1 = nn.BatchNorm1d(512)\n        self.batch_fc2 = nn.BatchNorm1d(256)\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)\n\n        self.dropout = nn.Dropout(p = p_dropout)\n\n    def forward(self, x:torch.Tensor) -> torch.Tensor:\n        x = self.pool(torch.relu(self.batch32(self.conv1(x))))\n        x = self.pool(torch.relu(self.batch64(self.conv2(x))))\n        x = self.pool(torch.relu(self.batch128(self.conv3(x))))\n        \n        x = x.flatten(1, -1)\n        \n        x = torch.relu(self.batch_fc1(self.fc1(x)))\n        x = self.dropout(x)\n        x = torch.relu(self.batch_fc2(self.fc2(x)))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n\ntrainloader, valloader, testloader = get_dataloaders_cnn(val_fraction = 0.2, num_workers = 4)\nmodel = ModifiedCNN(p_dropout = 0.25)\ntrain_metrics_modified_cnn = train_model_cnn(model, trainloader, valloader, num_epochs=30)\ntest_loss_modified_cnn, test_acc_modified_cnn = test_model(model, testloader)\nprint(f\"Test Loss: {test_loss_modified_cnn:.4f}, Test acc: {test_acc_modified_cnn * 100:.2f}%\")","metadata":{"id":"FXMesNkNsGGd","outputId":"2cf357bd-78b3-4fb9-8013-a8feb7e48540","execution":{"iopub.status.busy":"2024-05-07T12:29:48.644653Z","iopub.execute_input":"2024-05-07T12:29:48.645584Z","iopub.status.idle":"2024-05-07T12:54:59.561971Z","shell.execute_reply.started":"2024-05-07T12:29:48.645541Z","shell.execute_reply":"2024-05-07T12:54:59.560821Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nEpoch 0 learning rate [0.0009000000000000001]\n[Epoch 1] Train Loss: 1.3128, Val Loss: 1.0657, Train Accuracy: 52.83%, Val Accuracy: 62.28%\nEpoch 1 learning rate [0.0009000000000000001]\n[Epoch 2] Train Loss: 0.9998, Val Loss: 0.9341, Train Accuracy: 64.66%, Val Accuracy: 66.67%\nEpoch 2 learning rate [0.0009000000000000001]\n[Epoch 3] Train Loss: 0.8858, Val Loss: 0.8231, Train Accuracy: 68.81%, Val Accuracy: 70.82%\nEpoch 3 learning rate [0.0008100000000000001]\n[Epoch 4] Train Loss: 0.8078, Val Loss: 0.7938, Train Accuracy: 71.69%, Val Accuracy: 72.23%\nEpoch 4 learning rate [0.0008100000000000001]\n[Epoch 5] Train Loss: 0.7448, Val Loss: 0.7518, Train Accuracy: 73.92%, Val Accuracy: 73.51%\nEpoch 5 learning rate [0.0008100000000000001]\n[Epoch 6] Train Loss: 0.7028, Val Loss: 0.7018, Train Accuracy: 75.43%, Val Accuracy: 75.48%\nEpoch 6 learning rate [0.000729]\n[Epoch 7] Train Loss: 0.6685, Val Loss: 0.6931, Train Accuracy: 76.64%, Val Accuracy: 75.82%\nEpoch 7 learning rate [0.000729]\n[Epoch 8] Train Loss: 0.6270, Val Loss: 0.6495, Train Accuracy: 78.09%, Val Accuracy: 77.47%\nEpoch 8 learning rate [0.000729]\n[Epoch 9] Train Loss: 0.6019, Val Loss: 0.6424, Train Accuracy: 78.93%, Val Accuracy: 77.52%\nEpoch 9 learning rate [0.0006561000000000001]\nINFO: Early stopping counter 1 of 5\n[Epoch 10] Train Loss: 0.5816, Val Loss: 0.6462, Train Accuracy: 79.62%, Val Accuracy: 77.30%\nEpoch 10 learning rate [0.0006561000000000001]\n[Epoch 11] Train Loss: 0.5530, Val Loss: 0.5991, Train Accuracy: 80.65%, Val Accuracy: 79.03%\nEpoch 11 learning rate [0.0006561000000000001]\nINFO: Early stopping counter 1 of 5\n[Epoch 12] Train Loss: 0.5350, Val Loss: 0.6233, Train Accuracy: 81.16%, Val Accuracy: 78.55%\nEpoch 12 learning rate [0.00059049]\n[Epoch 13] Train Loss: 0.5168, Val Loss: 0.5863, Train Accuracy: 81.90%, Val Accuracy: 79.61%\nEpoch 13 learning rate [0.00059049]\n[Epoch 14] Train Loss: 0.4938, Val Loss: 0.5629, Train Accuracy: 82.62%, Val Accuracy: 80.64%\nEpoch 14 learning rate [0.00059049]\n[Epoch 15] Train Loss: 0.4815, Val Loss: 0.5559, Train Accuracy: 83.06%, Val Accuracy: 80.70%\nEpoch 15 learning rate [0.000531441]\n[Epoch 16] Train Loss: 0.4663, Val Loss: 0.5502, Train Accuracy: 83.76%, Val Accuracy: 80.91%\nEpoch 16 learning rate [0.000531441]\n[Epoch 17] Train Loss: 0.4470, Val Loss: 0.5286, Train Accuracy: 84.39%, Val Accuracy: 82.01%\nEpoch 17 learning rate [0.000531441]\n[Epoch 18] Train Loss: 0.4347, Val Loss: 0.5186, Train Accuracy: 84.85%, Val Accuracy: 81.99%\nEpoch 18 learning rate [0.0004782969]\nINFO: Early stopping counter 1 of 5\n[Epoch 19] Train Loss: 0.4236, Val Loss: 0.5238, Train Accuracy: 85.24%, Val Accuracy: 82.00%\nEpoch 19 learning rate [0.0004782969]\n[Epoch 20] Train Loss: 0.4104, Val Loss: 0.5100, Train Accuracy: 85.67%, Val Accuracy: 82.78%\nEpoch 20 learning rate [0.0004782969]\n[Epoch 21] Train Loss: 0.3955, Val Loss: 0.4980, Train Accuracy: 86.10%, Val Accuracy: 82.94%\nEpoch 21 learning rate [0.00043046721]\n[Epoch 22] Train Loss: 0.3921, Val Loss: 0.4913, Train Accuracy: 86.27%, Val Accuracy: 83.21%\nEpoch 22 learning rate [0.00043046721]\n[Epoch 23] Train Loss: 0.3794, Val Loss: 0.4856, Train Accuracy: 86.71%, Val Accuracy: 83.20%\nEpoch 23 learning rate [0.00043046721]\nINFO: Early stopping counter 1 of 5\n[Epoch 24] Train Loss: 0.3648, Val Loss: 0.4956, Train Accuracy: 87.23%, Val Accuracy: 83.10%\nEpoch 24 learning rate [0.000387420489]\nINFO: Early stopping counter 2 of 5\n[Epoch 25] Train Loss: 0.3645, Val Loss: 0.4998, Train Accuracy: 87.32%, Val Accuracy: 82.97%\nEpoch 25 learning rate [0.000387420489]\n[Epoch 26] Train Loss: 0.3526, Val Loss: 0.4788, Train Accuracy: 87.76%, Val Accuracy: 83.47%\nEpoch 26 learning rate [0.000387420489]\n[Epoch 27] Train Loss: 0.3458, Val Loss: 0.4769, Train Accuracy: 87.81%, Val Accuracy: 83.67%\nEpoch 27 learning rate [0.0003486784401]\n[Epoch 28] Train Loss: 0.3373, Val Loss: 0.4718, Train Accuracy: 88.14%, Val Accuracy: 83.79%\nEpoch 28 learning rate [0.0003486784401]\n[Epoch 29] Train Loss: 0.3291, Val Loss: 0.4637, Train Accuracy: 88.43%, Val Accuracy: 84.34%\nEpoch 29 learning rate [0.0003486784401]\nINFO: Early stopping counter 1 of 5\n[Epoch 30] Train Loss: 0.3221, Val Loss: 0.4710, Train Accuracy: 88.64%, Val Accuracy: 84.09%\nTest Loss: 0.5128, Test acc: 84.34%\nCPU times: user 2min 7s, sys: 24.2 s, total: 2min 32s\nWall time: 25min 10s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracy_loss(train_metrics_modified_cnn)","metadata":{"id":"4aOHH9EQFGYW","outputId":"d1145ad5-4f4f-4722-b3be-b951b486b822","execution":{"iopub.status.busy":"2024-05-06T06:50:22.431076Z","iopub.execute_input":"2024-05-06T06:50:22.431884Z","iopub.status.idle":"2024-05-06T06:50:22.455891Z","shell.execute_reply.started":"2024-05-06T06:50:22.431851Z","shell.execute_reply":"2024-05-06T06:50:22.454685Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_accuracy_loss\u001b[49m(train_metrics_modified_cnn)\n","\u001b[0;31mNameError\u001b[0m: name 'plot_accuracy_loss' is not defined"],"ename":"NameError","evalue":"name 'plot_accuracy_loss' is not defined","output_type":"error"}]},{"cell_type":"code","source":"fig = go.Figure()\nx = x=torch.arange(1, len(train_metrics_simple['train_acc']))\nfig.add_trace(go.Scatter(x=x, y=train_metrics_simple['val_acc'], mode='lines', name='Validation accuracy simple', opacity=0.7))\nfig.add_trace(go.Scatter(x=x, y=train_metrics_simple_cnn['val_acc'], mode='lines', name='Validation accuracy CNN', opacity=0.7))\nfig.add_trace(go.Scatter(x=x, y=train_metrics_modified_cnn['val_acc'], mode='lines', name='Validation accuracy mod CNN', opacity=0.7))\nfig.update_layout(title='Accuracy ', width=800, height=600, xaxis_title='Epoch', yaxis_title='Accuracy')\nfig.show()","metadata":{"id":"X0iBCSl1I2hR","outputId":"ed93ba03-b412-4308-ddd5-3d060a4b1554","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Решил ДЗ - дай обратную связь ;)\n\nНадеемся, было интересно и полезно.\n\nПожалуйста, оставьте обратную связь по этому домашнему заданию: https://forms.gle/iY5NRn9UfaZ344rbA","metadata":{"id":"ZQ1WSwJNerSj"}}]}