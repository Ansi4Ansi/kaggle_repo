{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install accelerate==0.25.0\n#!pip install bertopic==0.15.0\n#!pip install datasets==2.14.4\n#!pip install faiss-cpu==1.7.4\n#!pip install langchain==0.0.348\n#!pip install langchainhub==0.1.14\n#!pip install sentence-transformers==2.2.2\n#!pip install sentencepiece==0.1.99\n#!pip install transformers==4.24.0\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-18T19:05:33.619314Z","iopub.execute_input":"2024-05-18T19:05:33.619966Z","iopub.status.idle":"2024-05-18T19:05:46.709138Z","shell.execute_reply.started":"2024-05-18T19:05:33.619932Z","shell.execute_reply":"2024-05-18T19:05:46.707991Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:42:03.754874Z","iopub.execute_input":"2024-05-18T14:42:03.755238Z","iopub.status.idle":"2024-05-18T14:42:12.850681Z","shell.execute_reply.started":"2024-05-18T14:42:03.755210Z","shell.execute_reply":"2024-05-18T14:42:12.849763Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport transformers\nimport typing as tp\nimport wandb\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, PreTrainedTokenizer, PreTrainedTokenizerFast\nfrom datasets import load_dataset\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import recall_score, f1_score, precision_score\nfrom pprint import pprint\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntorch.manual_seed(42)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T19:32:08.386783Z","iopub.execute_input":"2024-05-18T19:32:08.387456Z","iopub.status.idle":"2024-05-18T19:32:08.398141Z","shell.execute_reply.started":"2024-05-18T19:32:08.387424Z","shell.execute_reply":"2024-05-18T19:32:08.397142Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7ccca9c29eb0>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    This class designs logic to retrieve data from a custom dataset.\n    According to pytorch Dataset conception any map style dataset\n    should implement at least __len__ and __getitem__ methods.\n    \"\"\"\n\n    def __init__(\n        self, texts, labels, tokenizer, max_length\n    ) -> None:\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self) -> int:\n        \"\"\"\n        returns number of rows in data\n        \"\"\"\n        return len(self.texts)\n\n    def __getitem__(self, idx: int) -> tp.Dict[str, tp.Any]:\n        \"\"\"\n        retrieves data for single index.\n        may include data processing and transformations.\n        E.g. augmenting data or tokenizing it.\n        returns dict with keys \"input_ids\", \"label\" and probably some more metadata (you decide whethere you need something more here)\n        \"\"\"\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\nclass ModelTrainer:\n    \"\"\"\n    This class implements logic run an experiemnt with a provided transformers classification model.\n    It incudes following components:\n    - load data\n    - load and configure a model and its artifacts\n    - train model\n    - validate model\n    - save model\n    - compue metrics\n    - run_experiment (as the man entrypoint to execute all flow)\n\n    Attention: current module intentionally doesnt support model inference or model serving.\n    It is a good practice to separate train/inference classes otherwise it is hard to maintain it all.\n\n    \"\"\"\n\n    def __init__(self, model_name: str, dataset_name: str) -> None:\n        self.model_name = model_name\n        self.dataset_name = dataset_name\n        \n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.model = transformers.AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=2)\n        \n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n        self.model_config = transformers.AutoConfig.from_pretrained(self.model_name)\n        \n        \n        \n\n    def configure_optimizer(self, optimizer, params: tp.Dict) -> None:\n        \"\"\"\n        adds a self.optimizer attribute with a chosen optimizer and its params.\n        \"\"\"\n\n        self.optimizer = optimizer\n        self.optimizer_params = params\n\n    def configure_scheduler(self, scheduler) -> None:\n        \"\"\"\n        adds a self.scheduler attribute with a chosen scheduler (e.g. ReduceLROnPlateau).\n        \"\"\"\n\n        self.scheduler = scheduler\n\n    def apply_data_parallel(self) -> None:\n        \"\"\"\n        checks number of available cuda devices,\n        if number of GPUs is > 1, moves self.model to a DataParallel state for faster training.\n        \"\"\"\n        if torch.cuda.device_count()>1:\n            net = torch.nn.DataParallel(self.model)\n\n    def load_data(self, filename: str, split: str) -> pd.DataFrame:\n        \"\"\"\n        uses Datasets library to load a dataset, takes as input dataset name (e.g. \"imdb\")\n        and a split. Loads data into pandas.\n        \"\"\"\n        ds = load_dataset(filename, split=split)\n        return ds.to_pandas()\n\n    def train(self, dataset: CustomDataset) -> None:\n        \n        \n        self.optimizer = self.optimizer or AdamW(self.model.parameters(), lr = 1e-3)\n        self.scheduler = self.scheduler or ExponentialLR(optimizer=self.optimizer, gamma=0.9, last_epoch=-1)\n        \n        train_loader = DataLoader(dataset, batch_size=8, shuffle=False)\n        \n        self.model.to(self.device)\n        \n        #pprint(wandb.config)\n        correct = 0\n        self.model.train()\n        for epoch in tqdm(range(wandb.config[\"epochs\"])):\n            total_loss = 0.0\n\n            for batch in train_loader:\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['label'].to(self.device)\n\n                self.optimizer.zero_grad()\n                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                loss.backward()\n                self.optimizer.step()\n                \n                if epoch % wandb.config[\"learning_rate_decay_step\"] == 0:\n                    self.scheduler.step()\n                #correct += (outputs[0].detach().cpu().numpy()==labels.to('cpu').numpy()).sum().item()\n                #pprint(correct)\n                total_loss += loss.item()\n            average_loss = total_loss / len(train_loader)\n            #average_accuracy = correct / len(train_loader)\n            wandb.log({\"accuracy\": average_accuracy, \"loss\": average_loss})\n            wandb.log({\"loss\": average_loss})\n            #print(f'Epoch {epoch+1}/{wandb.config[\"epochs\"]} - Average Loss: {average_loss:.4f} - Average accuracy {average_accuracy:.4f}')\n            print(f'Epoch {epoch+1}/{wandb.config[\"epochs\"]} - Average Loss: {average_loss:.4f}')\n\n        print(\"Training complete!\")\n\n    def validate(self, dataset: CustomDataset) -> tp.Dict[str, tp.Iterable]:\n        \"\"\"\n        takes a trained model and runs it on validation data.\n        Returns a dict with the keys \"valid_labels\" and \"valid_preds\" and corresponding values.\n        \"\"\"\n        \n        dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n        \n        model.to(self.device)\n        model.eval()\n        \n\n        valid_preds, valid_labels = [], []\n\n        for batch in dataloader:\n\n            b_input_ids = batch[\"input_ids\"].to(self.device)\n            b_input_mask = batch[\"attention_mask\"].to(self.device)\n            b_labels = batch[\"label\"].to(self.device)\n\n            with torch.no_grad():\n                logits = self.model(input_ids=b_input_ids, attention_mask=b_input_mask)\n\n            logits = logits[0].detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n\n            batch_preds = np.argmax(logits, axis=1)\n            batch_labels = np.concatenate(label_ids.reshape(-1,1))\n            valid_preds.extend(batch_preds)\n            valid_labels.extend(batch_labels)\n\n        return valid_labels, valid_preds\n\n    def compute_metrics_report(\n        self, labels: tp.Iterable, predictions: tp.Iterable\n    ) -> tp.Any:\n        \"\"\"\n        Computes classification metric (or several metrcis) for given task.\n        \"\"\"\n        \n        recall = recall_score(true_labels, true_predictions)\n        precision = precision_score(true_labels, true_predictions)\n        f1_score = f1_score(true_labels, true_predictions)\n\n        results = {\n            'recall': recall,\n            'precision': precision,\n            'f1': f1_score\n        }\n        return results\n\n    def save_model(self, dst_path: str) -> None:\n        \"\"\"\n        Saves model to dst_path. Be careful to check if a model is on DataParallel state.\n        If it is, one needs to process it accordingly.\n        \"\"\"\n        torch.save(self.model.state_dict(), dst_path)\n\n    def run_experiment(self):\n        \"\"\"\n        Main entrypoint.\n        Runs the flow from loading data to computing metrics.\n        \"\"\"\n        wandb.login()\n        \n        train_df = self.load_data(filename = self.dataset_name, split = 'train')\n        \n        self.apply_data_parallel()\n        \n        train_dataset = CustomDataset(\n            texts=train_df[\"text\"].tolist(),\n            labels=train_df[\"label\"].tolist(),\n            tokenizer=self.tokenizer,\n            max_length=512,\n        )\n        \n        wandb.init(project=\"hw-07-transformers\", notes=\"\", tags=[\"baseline\", \"transformer\", \"distilbert-base\"])\n        \n        wandb_params = {\n            \"lr\": 1e-3,\n            \"weight_decay\": 5e-4,\n            \"num_epoch\": 5,\n            \"learning_rate_decay_step\": 1,\n        }\n        \n        wandb.config = {\"epochs\":5, \"lr\": 1e-3, \"weight_decay\": 5e-4, \"learning_rate_decay_step\":1, \"learning_rate_decay_factor\":0.9,}\n        \n        self.configure_optimizer(optimizer = AdamW(self.model.parameters(), lr = 1e-3), params = wandb_params)    \n        self.configure_scheduler(scheduler = ExponentialLR(optimizer=self.optimizer, gamma=wandb.config[\"learning_rate_decay_factor\"], last_epoch=-1))\n        \n        self.train(train_dataset)\n        \n        test_df = self.load_data(filename = self.dataset_name, split = 'train')\n        \n        test_dataset = CustomDataset(\n            texts=test_df[\"text\"].tolist(),\n            labels=test_df[\"label\"].tolist(),\n            tokenizer=self.tokenizer,\n            max_length=512,\n        )\n        \n        valid_labels, valid_preds = self.validate(test_dataset)\n        \n        valid_scores = self.compute_metrics_report(valid_labels, valid_preds)\n        \n        self.save_model(dst_path = './distilbert-base-uncased_baseline.pt')\n        \n        pprint(valid_scores)\n        \n\nif __name__ == \"__main__\":\n    \"\"\"run experiment\"\"\"\n    model_trainer = ModelTrainer(model_name = \"distilbert-base-uncased\",\n                                 dataset_name = \"imdb\", \n                                )\n    model_trainer.run_experiment()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T20:46:27.586664Z","iopub.execute_input":"2024-05-18T20:46:27.587034Z","iopub.status.idle":"2024-05-18T21:10:22.872144Z","shell.execute_reply.started":"2024-05-18T20:46:27.587005Z","shell.execute_reply":"2024-05-18T21:10:22.870576Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:87i17ley) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.003 MB uploaded\\r'), FloatProgress(value=0.5202854996243426, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.0</td></tr><tr><td>loss</td><td>7.15439</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">giddy-butterfly-18</strong> at: <a href='https://wandb.ai/an4sim/hw-07-transformers/runs/87i17ley' target=\"_blank\">https://wandb.ai/an4sim/hw-07-transformers/runs/87i17ley</a><br/> View project at: <a href='https://wandb.ai/an4sim/hw-07-transformers' target=\"_blank\">https://wandb.ai/an4sim/hw-07-transformers</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240518_202401-87i17ley/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:87i17ley). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240518_204633-tpwhbdkl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/an4sim/hw-07-transformers/runs/tpwhbdkl' target=\"_blank\">silver-wind-19</a></strong> to <a href='https://wandb.ai/an4sim/hw-07-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/an4sim/hw-07-transformers' target=\"_blank\">https://wandb.ai/an4sim/hw-07-transformers</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/an4sim/hw-07-transformers/runs/tpwhbdkl' target=\"_blank\">https://wandb.ai/an4sim/hw-07-transformers/runs/tpwhbdkl</a>"},"metadata":{}},{"name":"stderr","text":" 20%|██        | 1/5 [11:50<47:21, 710.34s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 - Average Loss: 8.0456 - Average accuracy 0.0000\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 1/5 [23:28<1:33:52, 1408.05s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 261\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"run experiment\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m model_trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    259\u001b[0m                              dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    260\u001b[0m                             )\n\u001b[0;32m--> 261\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 236\u001b[0m, in \u001b[0;36mModelTrainer.run_experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigure_optimizer(optimizer \u001b[38;5;241m=\u001b[39m AdamW(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m), params \u001b[38;5;241m=\u001b[39m wandb_params)    \n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigure_scheduler(scheduler \u001b[38;5;241m=\u001b[39m ExponentialLR(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, gamma\u001b[38;5;241m=\u001b[39mwandb\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate_decay_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m], last_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data(filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    240\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\n\u001b[1;32m    241\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtest_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    242\u001b[0m     labels\u001b[38;5;241m=\u001b[39mtest_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    243\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m    244\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m    245\u001b[0m )\n","Cell \u001b[0;32mIn[31], line 136\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#_, preds = torch.max(outputs[0], dim=1)    \u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m#correct += (preds == labels).sum().item()\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m==\u001b[39mlabels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#pprint(correct)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}